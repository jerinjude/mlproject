{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import scikit-learn\n",
    "import os\n",
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import mlflow\n",
    "from mlflow import log_metric, log_param, log_params, log_artifacts\n",
    "#import tensorflow\n",
    "#from tensorflow.summary import create_file_writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH='/home/vboxuser/mlprojects/data/made_with_ml'\n",
    "MODEL_NAME = 'bert-base-nli-mean-tokens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_process_data(parent_dir,filename):\n",
    "    '''\n",
    "    This function will read and prepare text for training\n",
    "    '''\n",
    "    df=pd.read_csv(os.path.join(parent_dir,filename),dtype={'title':str,'description':str,'tag':str},index_col=None)\n",
    "    df['combined']=df['title'] + ' ' + df['description']\n",
    "    df['combined']=df['combined'].apply(lambda text:text.lower())\n",
    "    df['combined_with_SEP']=df['title'] + ' [SEP] ' + df['description']\n",
    "    df['combined_with_SEP']=df['combined_with_SEP'].apply(lambda text:text.lower())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=read_and_process_data(DATA_PATH,'dataset.csv')\n",
    "df_valid=read_and_process_data(DATA_PATH,'holdout.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>created_on</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>tag</th>\n",
       "      <th>combined</th>\n",
       "      <th>combined_with_SEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>2020-02-20 06:43:18</td>\n",
       "      <td>Comparison between YOLO and RCNN on real world...</td>\n",
       "      <td>Bringing theory to experiment is cool. We can ...</td>\n",
       "      <td>computer-vision</td>\n",
       "      <td>comparison between yolo and rcnn on real world...</td>\n",
       "      <td>comparison between yolo and rcnn on real world...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>2020-02-20 06:47:21</td>\n",
       "      <td>Show, Infer &amp; Tell: Contextual Inference for C...</td>\n",
       "      <td>The beauty of the work lies in the way it arch...</td>\n",
       "      <td>computer-vision</td>\n",
       "      <td>show, infer &amp; tell: contextual inference for c...</td>\n",
       "      <td>show, infer &amp; tell: contextual inference for c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>2020-02-24 16:24:45</td>\n",
       "      <td>Awesome Graph Classification</td>\n",
       "      <td>A collection of important graph embedding, cla...</td>\n",
       "      <td>graph-learning</td>\n",
       "      <td>awesome graph classification a collection of i...</td>\n",
       "      <td>awesome graph classification [sep] a collectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>2020-02-28 23:55:26</td>\n",
       "      <td>Awesome Monte Carlo Tree Search</td>\n",
       "      <td>A curated list of Monte Carlo tree search pape...</td>\n",
       "      <td>reinforcement-learning</td>\n",
       "      <td>awesome monte carlo tree search a curated list...</td>\n",
       "      <td>awesome monte carlo tree search [sep] a curate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>2020-03-07 23:04:31</td>\n",
       "      <td>AttentionWalk</td>\n",
       "      <td>A PyTorch Implementation of \"Watch Your Step: ...</td>\n",
       "      <td>graph-learning</td>\n",
       "      <td>attentionwalk a pytorch implementation of \"wat...</td>\n",
       "      <td>attentionwalk [sep] a pytorch implementation o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id           created_on                                              title  \\\n",
       "0   6  2020-02-20 06:43:18  Comparison between YOLO and RCNN on real world...   \n",
       "1   7  2020-02-20 06:47:21  Show, Infer & Tell: Contextual Inference for C...   \n",
       "2   9  2020-02-24 16:24:45                       Awesome Graph Classification   \n",
       "3  15  2020-02-28 23:55:26                    Awesome Monte Carlo Tree Search   \n",
       "4  25  2020-03-07 23:04:31                                      AttentionWalk   \n",
       "\n",
       "                                         description                     tag  \\\n",
       "0  Bringing theory to experiment is cool. We can ...         computer-vision   \n",
       "1  The beauty of the work lies in the way it arch...         computer-vision   \n",
       "2  A collection of important graph embedding, cla...          graph-learning   \n",
       "3  A curated list of Monte Carlo tree search pape...  reinforcement-learning   \n",
       "4  A PyTorch Implementation of \"Watch Your Step: ...          graph-learning   \n",
       "\n",
       "                                            combined  \\\n",
       "0  comparison between yolo and rcnn on real world...   \n",
       "1  show, infer & tell: contextual inference for c...   \n",
       "2  awesome graph classification a collection of i...   \n",
       "3  awesome monte carlo tree search a curated list...   \n",
       "4  attentionwalk a pytorch implementation of \"wat...   \n",
       "\n",
       "                                   combined_with_SEP  \n",
       "0  comparison between yolo and rcnn on real world...  \n",
       "1  show, infer & tell: contextual inference for c...  \n",
       "2  awesome graph classification [sep] a collectio...  \n",
       "3  awesome monte carlo tree search [sep] a curate...  \n",
       "4  attentionwalk [sep] a pytorch implementation o...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['computer-vision', 'graph-learning', 'reinforcement-learning',\n",
       "       'natural-language-processing', 'mlops', 'time-series'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['tag'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(model_name='sentence-transformers/all-mpnet-base-v2'):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurization(text,model):\n",
    "    embeddings = model.encode(text)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(self):\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(self.df['tag'])\n",
    "    return le\n",
    "\n",
    "def label_transform(df,le):\n",
    "    return le.transform(df['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_init(model_name='sentence-transformers/all-mpnet-base-v2')\n",
    "df_train['bert_features_combined']=df_train['combined'].apply(lambda x: featurization(x,model))\n",
    "df_train['bert_features_combined_SEP']=df_train['combined_with_SEP'].apply(lambda x: featurization(x,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid['bert_features_combined']=df_valid['combined'].apply(lambda x: featurization(x,model))\n",
    "df_valid['bert_features_combined_SEP']=df_valid['combined_with_SEP'].apply(lambda x: featurization(x,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(df_train['tag'])\n",
    "df_train['label_int'] = label_encoder.transform(df_train['tag'])\n",
    "df_valid['label_int'] = label_encoder.transform(df_valid['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneHotEncoder(handle_unknown='ignore')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le = OneHotEncoder(handle_unknown='ignore')\n",
    "#le.fit(df_train['label_int'])\n",
    "le.fit(df_train['label_int'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13866/1907311675.py:12: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /croot/pytorch_1686931851744/work/torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  X_train=torch.tensor(df_train['bert_features_combined'])\n"
     ]
    }
   ],
   "source": [
    "label_int_train = df_train['label_int'].values.reshape(-1, 1)\n",
    "label_int_valid = df_valid['label_int'].values.reshape(-1, 1)\n",
    "\n",
    "# Create and fit OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "onehot_encoder.fit(label_int_train)\n",
    "\n",
    "# Transform and convert to DataFrame\n",
    "y_train = torch.tensor(onehot_encoder.transform(label_int_train).toarray())\n",
    "y_valid = torch.tensor(onehot_encoder.transform(label_int_valid).toarray())\n",
    "\n",
    "X_train=torch.tensor(df_train['bert_features_combined'])\n",
    "X_valid=torch.tensor(df_valid['bert_features_combined'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#log_dir = \"logs\"  # Specify the log directory path\n",
    "#writer = create_file_writer(log_dir)\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/16 16:58:13 INFO mlflow.tracking.fluent: Experiment with name 'topic_classification' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///home/vboxuser/mlprojects/sample/notebooks/mlruns/728078586741920700', creation_time=1692219493226, experiment_id='728078586741920700', last_update_time=1692219493226, lifecycle_stage='active', name='topic_classification', tags={}>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(\"topic_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as run:\n",
    "    run_id = run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000] - Training Loss: 1.6448\n",
      "Epoch [1/1000] - Validation Loss: 1.6424\n",
      "Epoch [2/1000] - Training Loss: 1.4017\n",
      "Epoch [2/1000] - Validation Loss: 1.3928\n",
      "Epoch [3/1000] - Training Loss: 1.3241\n",
      "Epoch [3/1000] - Validation Loss: 1.3122\n",
      "Epoch [4/1000] - Training Loss: 1.3029\n",
      "Epoch [4/1000] - Validation Loss: 1.2918\n",
      "Epoch [5/1000] - Training Loss: 1.2892\n",
      "Epoch [5/1000] - Validation Loss: 1.2805\n",
      "Epoch [6/1000] - Training Loss: 1.2623\n",
      "Epoch [6/1000] - Validation Loss: 1.2537\n",
      "Epoch [7/1000] - Training Loss: 1.2372\n",
      "Epoch [7/1000] - Validation Loss: 1.2298\n",
      "Epoch [8/1000] - Training Loss: 1.2231\n",
      "Epoch [8/1000] - Validation Loss: 1.2168\n",
      "Epoch [9/1000] - Training Loss: 1.2090\n",
      "Epoch [9/1000] - Validation Loss: 1.2039\n",
      "Epoch [10/1000] - Training Loss: 1.1908\n",
      "Epoch [10/1000] - Validation Loss: 1.1850\n",
      "Epoch [11/1000] - Training Loss: 1.1757\n",
      "Epoch [11/1000] - Validation Loss: 1.1730\n",
      "Epoch [12/1000] - Training Loss: 1.1612\n",
      "Epoch [12/1000] - Validation Loss: 1.1639\n",
      "Epoch [13/1000] - Training Loss: 1.1476\n",
      "Epoch [13/1000] - Validation Loss: 1.1550\n",
      "Epoch [14/1000] - Training Loss: 1.1299\n",
      "Epoch [14/1000] - Validation Loss: 1.1402\n",
      "Epoch [15/1000] - Training Loss: 1.1137\n",
      "Epoch [15/1000] - Validation Loss: 1.1243\n",
      "Epoch [16/1000] - Training Loss: 1.1024\n",
      "Epoch [16/1000] - Validation Loss: 1.1134\n",
      "Epoch [17/1000] - Training Loss: 1.0954\n",
      "Epoch [17/1000] - Validation Loss: 1.1075\n",
      "Epoch [18/1000] - Training Loss: 1.0911\n",
      "Epoch [18/1000] - Validation Loss: 1.1048\n",
      "Epoch [19/1000] - Training Loss: 1.0871\n",
      "Epoch [19/1000] - Validation Loss: 1.1023\n",
      "Epoch [20/1000] - Training Loss: 1.0838\n",
      "Epoch [20/1000] - Validation Loss: 1.0989\n",
      "Epoch [21/1000] - Training Loss: 1.0811\n",
      "Epoch [21/1000] - Validation Loss: 1.0963\n",
      "Epoch [22/1000] - Training Loss: 1.0795\n",
      "Epoch [22/1000] - Validation Loss: 1.0950\n",
      "Epoch [23/1000] - Training Loss: 1.0770\n",
      "Epoch [23/1000] - Validation Loss: 1.0946\n",
      "Epoch [24/1000] - Training Loss: 1.0755\n",
      "Epoch [24/1000] - Validation Loss: 1.0939\n",
      "Epoch [25/1000] - Training Loss: 1.0738\n",
      "Epoch [25/1000] - Validation Loss: 1.0939\n",
      "Epoch [26/1000] - Training Loss: 1.0725\n",
      "Epoch [26/1000] - Validation Loss: 1.0924\n",
      "Epoch [27/1000] - Training Loss: 1.0710\n",
      "Epoch [27/1000] - Validation Loss: 1.0920\n",
      "Epoch [28/1000] - Training Loss: 1.0697\n",
      "Epoch [28/1000] - Validation Loss: 1.0924\n",
      "Epoch [29/1000] - Training Loss: 1.0690\n",
      "Epoch [29/1000] - Validation Loss: 1.0929\n",
      "Epoch [30/1000] - Training Loss: 1.0679\n",
      "Epoch [30/1000] - Validation Loss: 1.0921\n",
      "Epoch [31/1000] - Training Loss: 1.0671\n",
      "Epoch [31/1000] - Validation Loss: 1.0911\n",
      "Epoch [32/1000] - Training Loss: 1.0667\n",
      "Epoch [32/1000] - Validation Loss: 1.0908\n",
      "Epoch [33/1000] - Training Loss: 1.0657\n",
      "Epoch [33/1000] - Validation Loss: 1.0912\n",
      "Epoch [34/1000] - Training Loss: 1.0651\n",
      "Epoch [34/1000] - Validation Loss: 1.0913\n",
      "Epoch [35/1000] - Training Loss: 1.0646\n",
      "Epoch [35/1000] - Validation Loss: 1.0913\n",
      "Epoch [36/1000] - Training Loss: 1.0639\n",
      "Epoch [36/1000] - Validation Loss: 1.0899\n",
      "Epoch [37/1000] - Training Loss: 1.0628\n",
      "Epoch [37/1000] - Validation Loss: 1.0901\n",
      "Epoch [38/1000] - Training Loss: 1.0623\n",
      "Epoch [38/1000] - Validation Loss: 1.0892\n",
      "Epoch [39/1000] - Training Loss: 1.0618\n",
      "Epoch [39/1000] - Validation Loss: 1.0905\n",
      "Epoch [40/1000] - Training Loss: 1.0612\n",
      "Epoch [40/1000] - Validation Loss: 1.0914\n",
      "Epoch [41/1000] - Training Loss: 1.0604\n",
      "Epoch [41/1000] - Validation Loss: 1.0894\n",
      "Epoch [42/1000] - Training Loss: 1.0593\n",
      "Epoch [42/1000] - Validation Loss: 1.0905\n",
      "Epoch [43/1000] - Training Loss: 1.0584\n",
      "Epoch [43/1000] - Validation Loss: 1.0897\n",
      "Epoch [44/1000] - Training Loss: 1.0581\n",
      "Epoch [44/1000] - Validation Loss: 1.0889\n",
      "Epoch [45/1000] - Training Loss: 1.0576\n",
      "Epoch [45/1000] - Validation Loss: 1.0901\n",
      "Epoch [46/1000] - Training Loss: 1.0571\n",
      "Epoch [46/1000] - Validation Loss: 1.0899\n",
      "Epoch [47/1000] - Training Loss: 1.0569\n",
      "Epoch [47/1000] - Validation Loss: 1.0884\n",
      "Epoch [48/1000] - Training Loss: 1.0564\n",
      "Epoch [48/1000] - Validation Loss: 1.0901\n",
      "Epoch [49/1000] - Training Loss: 1.0560\n",
      "Epoch [49/1000] - Validation Loss: 1.0887\n",
      "Epoch [50/1000] - Training Loss: 1.0552\n",
      "Epoch [50/1000] - Validation Loss: 1.0885\n",
      "Epoch [51/1000] - Training Loss: 1.0547\n",
      "Epoch [51/1000] - Validation Loss: 1.0896\n",
      "Epoch [52/1000] - Training Loss: 1.0541\n",
      "Epoch [52/1000] - Validation Loss: 1.0884\n",
      "Epoch [53/1000] - Training Loss: 1.0535\n",
      "Epoch [53/1000] - Validation Loss: 1.0893\n",
      "Epoch [54/1000] - Training Loss: 1.0532\n",
      "Epoch [54/1000] - Validation Loss: 1.0890\n",
      "Epoch [55/1000] - Training Loss: 1.0532\n",
      "Epoch [55/1000] - Validation Loss: 1.0894\n",
      "Epoch [56/1000] - Training Loss: 1.0528\n",
      "Epoch [56/1000] - Validation Loss: 1.0875\n",
      "Epoch [57/1000] - Training Loss: 1.0522\n",
      "Epoch [57/1000] - Validation Loss: 1.0879\n",
      "Epoch [58/1000] - Training Loss: 1.0515\n",
      "Epoch [58/1000] - Validation Loss: 1.0887\n",
      "Epoch [59/1000] - Training Loss: 1.0511\n",
      "Epoch [59/1000] - Validation Loss: 1.0875\n",
      "Epoch [60/1000] - Training Loss: 1.0504\n",
      "Epoch [60/1000] - Validation Loss: 1.0885\n",
      "Epoch [61/1000] - Training Loss: 1.0498\n",
      "Epoch [61/1000] - Validation Loss: 1.0875\n",
      "Epoch [62/1000] - Training Loss: 1.0496\n",
      "Epoch [62/1000] - Validation Loss: 1.0854\n",
      "Epoch [63/1000] - Training Loss: 1.0493\n",
      "Epoch [63/1000] - Validation Loss: 1.0860\n",
      "Epoch [64/1000] - Training Loss: 1.0490\n",
      "Epoch [64/1000] - Validation Loss: 1.0845\n",
      "Epoch [65/1000] - Training Loss: 1.0485\n",
      "Epoch [65/1000] - Validation Loss: 1.0824\n",
      "Epoch [66/1000] - Training Loss: 1.0481\n",
      "Epoch [66/1000] - Validation Loss: 1.0833\n",
      "Epoch [67/1000] - Training Loss: 1.0479\n",
      "Epoch [67/1000] - Validation Loss: 1.0833\n",
      "Epoch [68/1000] - Training Loss: 1.0476\n",
      "Epoch [68/1000] - Validation Loss: 1.0824\n",
      "Epoch [69/1000] - Training Loss: 1.0475\n",
      "Epoch [69/1000] - Validation Loss: 1.0819\n",
      "Epoch [70/1000] - Training Loss: 1.0473\n",
      "Epoch [70/1000] - Validation Loss: 1.0819\n",
      "Epoch [71/1000] - Training Loss: 1.0475\n",
      "Epoch [71/1000] - Validation Loss: 1.0834\n",
      "Epoch [72/1000] - Training Loss: 1.0472\n",
      "Epoch [72/1000] - Validation Loss: 1.0827\n",
      "Epoch [73/1000] - Training Loss: 1.0472\n",
      "Epoch [73/1000] - Validation Loss: 1.0814\n",
      "Epoch [74/1000] - Training Loss: 1.0471\n",
      "Epoch [74/1000] - Validation Loss: 1.0831\n",
      "Epoch [75/1000] - Training Loss: 1.0471\n",
      "Epoch [75/1000] - Validation Loss: 1.0835\n",
      "Epoch [76/1000] - Training Loss: 1.0470\n",
      "Epoch [76/1000] - Validation Loss: 1.0822\n",
      "Epoch [77/1000] - Training Loss: 1.0470\n",
      "Epoch [77/1000] - Validation Loss: 1.0835\n",
      "Epoch [78/1000] - Training Loss: 1.0469\n",
      "Epoch [78/1000] - Validation Loss: 1.0825\n",
      "Epoch [79/1000] - Training Loss: 1.0469\n",
      "Epoch [79/1000] - Validation Loss: 1.0821\n",
      "Epoch [80/1000] - Training Loss: 1.0468\n",
      "Epoch [80/1000] - Validation Loss: 1.0823\n",
      "Epoch [81/1000] - Training Loss: 1.0468\n",
      "Epoch [81/1000] - Validation Loss: 1.0825\n",
      "Epoch [82/1000] - Training Loss: 1.0467\n",
      "Epoch [82/1000] - Validation Loss: 1.0817\n",
      "Epoch [83/1000] - Training Loss: 1.0467\n",
      "Epoch [83/1000] - Validation Loss: 1.0812\n",
      "Epoch [84/1000] - Training Loss: 1.0467\n",
      "Epoch [84/1000] - Validation Loss: 1.0810\n",
      "Epoch [85/1000] - Training Loss: 1.0466\n",
      "Epoch [85/1000] - Validation Loss: 1.0811\n",
      "Epoch [86/1000] - Training Loss: 1.0465\n",
      "Epoch [86/1000] - Validation Loss: 1.0812\n",
      "Epoch [87/1000] - Training Loss: 1.0464\n",
      "Epoch [87/1000] - Validation Loss: 1.0817\n",
      "Epoch [88/1000] - Training Loss: 1.0462\n",
      "Epoch [88/1000] - Validation Loss: 1.0811\n",
      "Epoch [89/1000] - Training Loss: 1.0461\n",
      "Epoch [89/1000] - Validation Loss: 1.0810\n",
      "Epoch [90/1000] - Training Loss: 1.0458\n",
      "Epoch [90/1000] - Validation Loss: 1.0801\n",
      "Epoch [91/1000] - Training Loss: 1.0456\n",
      "Epoch [91/1000] - Validation Loss: 1.0806\n",
      "Epoch [92/1000] - Training Loss: 1.0456\n",
      "Epoch [92/1000] - Validation Loss: 1.0815\n",
      "Epoch [93/1000] - Training Loss: 1.0455\n",
      "Epoch [93/1000] - Validation Loss: 1.0817\n",
      "Epoch [94/1000] - Training Loss: 1.0455\n",
      "Epoch [94/1000] - Validation Loss: 1.0813\n",
      "Epoch [95/1000] - Training Loss: 1.0456\n",
      "Epoch [95/1000] - Validation Loss: 1.0812\n",
      "Epoch [96/1000] - Training Loss: 1.0454\n",
      "Epoch [96/1000] - Validation Loss: 1.0811\n",
      "Epoch [97/1000] - Training Loss: 1.0454\n",
      "Epoch [97/1000] - Validation Loss: 1.0814\n",
      "Epoch [98/1000] - Training Loss: 1.0454\n",
      "Epoch [98/1000] - Validation Loss: 1.0804\n",
      "Epoch [99/1000] - Training Loss: 1.0453\n",
      "Epoch [99/1000] - Validation Loss: 1.0815\n",
      "Epoch [100/1000] - Training Loss: 1.0453\n",
      "Epoch [100/1000] - Validation Loss: 1.0821\n",
      "Epoch [101/1000] - Training Loss: 1.0453\n",
      "Epoch [101/1000] - Validation Loss: 1.0822\n",
      "Epoch [102/1000] - Training Loss: 1.0453\n",
      "Epoch [102/1000] - Validation Loss: 1.0817\n",
      "Epoch [103/1000] - Training Loss: 1.0453\n",
      "Epoch [103/1000] - Validation Loss: 1.0808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [104/1000] - Training Loss: 1.0453\n",
      "Epoch [104/1000] - Validation Loss: 1.0804\n",
      "Epoch [105/1000] - Training Loss: 1.0453\n",
      "Epoch [105/1000] - Validation Loss: 1.0805\n",
      "Epoch [106/1000] - Training Loss: 1.0452\n",
      "Epoch [106/1000] - Validation Loss: 1.0813\n",
      "Epoch [107/1000] - Training Loss: 1.0452\n",
      "Epoch [107/1000] - Validation Loss: 1.0813\n",
      "Epoch [108/1000] - Training Loss: 1.0452\n",
      "Epoch [108/1000] - Validation Loss: 1.0808\n",
      "Epoch [109/1000] - Training Loss: 1.0452\n",
      "Epoch [109/1000] - Validation Loss: 1.0809\n",
      "Epoch [110/1000] - Training Loss: 1.0452\n",
      "Epoch [110/1000] - Validation Loss: 1.0808\n",
      "Epoch [111/1000] - Training Loss: 1.0452\n",
      "Epoch [111/1000] - Validation Loss: 1.0806\n",
      "Epoch [112/1000] - Training Loss: 1.0452\n",
      "Epoch [112/1000] - Validation Loss: 1.0806\n",
      "Epoch [113/1000] - Training Loss: 1.0452\n",
      "Epoch [113/1000] - Validation Loss: 1.0808\n",
      "Epoch [114/1000] - Training Loss: 1.0452\n",
      "Epoch [114/1000] - Validation Loss: 1.0809\n",
      "Epoch [115/1000] - Training Loss: 1.0452\n",
      "Epoch [115/1000] - Validation Loss: 1.0809\n",
      "Epoch [116/1000] - Training Loss: 1.0451\n",
      "Epoch [116/1000] - Validation Loss: 1.0811\n",
      "Epoch [117/1000] - Training Loss: 1.0451\n",
      "Epoch [117/1000] - Validation Loss: 1.0809\n",
      "Epoch [118/1000] - Training Loss: 1.0451\n",
      "Epoch [118/1000] - Validation Loss: 1.0820\n",
      "Epoch [119/1000] - Training Loss: 1.0451\n",
      "Epoch [119/1000] - Validation Loss: 1.0820\n",
      "Epoch [120/1000] - Training Loss: 1.0451\n",
      "Epoch [120/1000] - Validation Loss: 1.0810\n",
      "Epoch [121/1000] - Training Loss: 1.0451\n",
      "Epoch [121/1000] - Validation Loss: 1.0813\n",
      "Epoch [122/1000] - Training Loss: 1.0451\n",
      "Epoch [122/1000] - Validation Loss: 1.0811\n",
      "Epoch [123/1000] - Training Loss: 1.0451\n",
      "Epoch [123/1000] - Validation Loss: 1.0817\n",
      "Epoch [124/1000] - Training Loss: 1.0451\n",
      "Epoch [124/1000] - Validation Loss: 1.0818\n",
      "Epoch [125/1000] - Training Loss: 1.0451\n",
      "Epoch [125/1000] - Validation Loss: 1.0818\n",
      "Epoch [126/1000] - Training Loss: 1.0451\n",
      "Epoch [126/1000] - Validation Loss: 1.0817\n",
      "Epoch [127/1000] - Training Loss: 1.0451\n",
      "Epoch [127/1000] - Validation Loss: 1.0827\n",
      "Epoch [128/1000] - Training Loss: 1.0451\n",
      "Epoch [128/1000] - Validation Loss: 1.0826\n",
      "Epoch [129/1000] - Training Loss: 1.0452\n",
      "Epoch [129/1000] - Validation Loss: 1.0820\n",
      "Epoch [130/1000] - Training Loss: 1.0451\n",
      "Epoch [130/1000] - Validation Loss: 1.0816\n",
      "Epoch [131/1000] - Training Loss: 1.0450\n",
      "Epoch [131/1000] - Validation Loss: 1.0819\n",
      "Epoch [132/1000] - Training Loss: 1.0451\n",
      "Epoch [132/1000] - Validation Loss: 1.0814\n",
      "Epoch [133/1000] - Training Loss: 1.0451\n",
      "Epoch [133/1000] - Validation Loss: 1.0821\n",
      "Epoch [134/1000] - Training Loss: 1.0450\n",
      "Epoch [134/1000] - Validation Loss: 1.0824\n",
      "Epoch [135/1000] - Training Loss: 1.0450\n",
      "Epoch [135/1000] - Validation Loss: 1.0834\n",
      "Epoch [136/1000] - Training Loss: 1.0450\n",
      "Epoch [136/1000] - Validation Loss: 1.0843\n",
      "Epoch [137/1000] - Training Loss: 1.0450\n",
      "Epoch [137/1000] - Validation Loss: 1.0836\n",
      "Epoch [138/1000] - Training Loss: 1.0450\n",
      "Epoch [138/1000] - Validation Loss: 1.0833\n",
      "Epoch [139/1000] - Training Loss: 1.0450\n",
      "Epoch [139/1000] - Validation Loss: 1.0836\n",
      "Epoch [140/1000] - Training Loss: 1.0450\n",
      "Epoch [140/1000] - Validation Loss: 1.0832\n",
      "Epoch [141/1000] - Training Loss: 1.0449\n",
      "Epoch [141/1000] - Validation Loss: 1.0842\n",
      "Epoch [142/1000] - Training Loss: 1.0449\n",
      "Epoch [142/1000] - Validation Loss: 1.0845\n",
      "Epoch [143/1000] - Training Loss: 1.0449\n",
      "Epoch [143/1000] - Validation Loss: 1.0846\n",
      "Epoch [144/1000] - Training Loss: 1.0449\n",
      "Epoch [144/1000] - Validation Loss: 1.0844\n",
      "Epoch [145/1000] - Training Loss: 1.0448\n",
      "Epoch [145/1000] - Validation Loss: 1.0863\n",
      "Epoch [146/1000] - Training Loss: 1.0445\n",
      "Epoch [146/1000] - Validation Loss: 1.0860\n",
      "Epoch [147/1000] - Training Loss: 1.0442\n",
      "Epoch [147/1000] - Validation Loss: 1.0856\n",
      "Epoch [148/1000] - Training Loss: 1.0438\n",
      "Epoch [148/1000] - Validation Loss: 1.0885\n",
      "Epoch [149/1000] - Training Loss: 1.0438\n",
      "Epoch [149/1000] - Validation Loss: 1.0869\n",
      "Epoch [150/1000] - Training Loss: 1.0438\n",
      "Epoch [150/1000] - Validation Loss: 1.0859\n",
      "Epoch [151/1000] - Training Loss: 1.0438\n",
      "Epoch [151/1000] - Validation Loss: 1.0874\n",
      "Epoch [152/1000] - Training Loss: 1.0437\n",
      "Epoch [152/1000] - Validation Loss: 1.0871\n",
      "Epoch [153/1000] - Training Loss: 1.0437\n",
      "Epoch [153/1000] - Validation Loss: 1.0859\n",
      "Epoch [154/1000] - Training Loss: 1.0437\n",
      "Epoch [154/1000] - Validation Loss: 1.0855\n",
      "Epoch [155/1000] - Training Loss: 1.0437\n",
      "Epoch [155/1000] - Validation Loss: 1.0844\n",
      "Epoch [156/1000] - Training Loss: 1.0437\n",
      "Epoch [156/1000] - Validation Loss: 1.0848\n",
      "Epoch [157/1000] - Training Loss: 1.0437\n",
      "Epoch [157/1000] - Validation Loss: 1.0847\n",
      "Epoch [158/1000] - Training Loss: 1.0437\n",
      "Epoch [158/1000] - Validation Loss: 1.0846\n",
      "Epoch [159/1000] - Training Loss: 1.0437\n",
      "Epoch [159/1000] - Validation Loss: 1.0845\n",
      "Epoch [160/1000] - Training Loss: 1.0437\n",
      "Epoch [160/1000] - Validation Loss: 1.0840\n",
      "Epoch [161/1000] - Training Loss: 1.0437\n",
      "Epoch [161/1000] - Validation Loss: 1.0843\n",
      "Epoch [162/1000] - Training Loss: 1.0437\n",
      "Epoch [162/1000] - Validation Loss: 1.0842\n",
      "Epoch [163/1000] - Training Loss: 1.0437\n",
      "Epoch [163/1000] - Validation Loss: 1.0843\n",
      "Epoch [164/1000] - Training Loss: 1.0437\n",
      "Epoch [164/1000] - Validation Loss: 1.0842\n",
      "Epoch [165/1000] - Training Loss: 1.0437\n",
      "Epoch [165/1000] - Validation Loss: 1.0849\n",
      "Epoch [166/1000] - Training Loss: 1.0437\n",
      "Epoch [166/1000] - Validation Loss: 1.0849\n",
      "Epoch [167/1000] - Training Loss: 1.0437\n",
      "Epoch [167/1000] - Validation Loss: 1.0859\n",
      "Epoch [168/1000] - Training Loss: 1.0437\n",
      "Epoch [168/1000] - Validation Loss: 1.0855\n",
      "Epoch [169/1000] - Training Loss: 1.0437\n",
      "Epoch [169/1000] - Validation Loss: 1.0844\n",
      "Epoch [170/1000] - Training Loss: 1.0437\n",
      "Epoch [170/1000] - Validation Loss: 1.0849\n",
      "Epoch [171/1000] - Training Loss: 1.0437\n",
      "Epoch [171/1000] - Validation Loss: 1.0846\n",
      "Epoch [172/1000] - Training Loss: 1.0437\n",
      "Epoch [172/1000] - Validation Loss: 1.0844\n",
      "Epoch [173/1000] - Training Loss: 1.0437\n",
      "Epoch [173/1000] - Validation Loss: 1.0842\n",
      "Epoch [174/1000] - Training Loss: 1.0437\n",
      "Epoch [174/1000] - Validation Loss: 1.0843\n",
      "Epoch [175/1000] - Training Loss: 1.0437\n",
      "Epoch [175/1000] - Validation Loss: 1.0842\n",
      "Epoch [176/1000] - Training Loss: 1.0437\n",
      "Epoch [176/1000] - Validation Loss: 1.0844\n",
      "Epoch [177/1000] - Training Loss: 1.0437\n",
      "Epoch [177/1000] - Validation Loss: 1.0845\n",
      "Epoch [178/1000] - Training Loss: 1.0437\n",
      "Epoch [178/1000] - Validation Loss: 1.0850\n",
      "Epoch [179/1000] - Training Loss: 1.0437\n",
      "Epoch [179/1000] - Validation Loss: 1.0847\n",
      "Epoch [180/1000] - Training Loss: 1.0437\n",
      "Epoch [180/1000] - Validation Loss: 1.0844\n",
      "Epoch [181/1000] - Training Loss: 1.0437\n",
      "Epoch [181/1000] - Validation Loss: 1.0843\n",
      "Epoch [182/1000] - Training Loss: 1.0437\n",
      "Epoch [182/1000] - Validation Loss: 1.0855\n",
      "Epoch [183/1000] - Training Loss: 1.0437\n",
      "Epoch [183/1000] - Validation Loss: 1.0854\n",
      "Epoch [184/1000] - Training Loss: 1.0437\n",
      "Epoch [184/1000] - Validation Loss: 1.0848\n",
      "Epoch [185/1000] - Training Loss: 1.0436\n",
      "Epoch [185/1000] - Validation Loss: 1.0844\n",
      "Epoch [186/1000] - Training Loss: 1.0436\n",
      "Epoch [186/1000] - Validation Loss: 1.0839\n",
      "Epoch [187/1000] - Training Loss: 1.0436\n",
      "Epoch [187/1000] - Validation Loss: 1.0844\n",
      "Epoch [188/1000] - Training Loss: 1.0436\n",
      "Epoch [188/1000] - Validation Loss: 1.0842\n",
      "Epoch [189/1000] - Training Loss: 1.0436\n",
      "Epoch [189/1000] - Validation Loss: 1.0844\n",
      "Epoch [190/1000] - Training Loss: 1.0436\n",
      "Epoch [190/1000] - Validation Loss: 1.0841\n",
      "Epoch [191/1000] - Training Loss: 1.0436\n",
      "Epoch [191/1000] - Validation Loss: 1.0843\n",
      "Epoch [192/1000] - Training Loss: 1.0436\n",
      "Epoch [192/1000] - Validation Loss: 1.0838\n",
      "Epoch [193/1000] - Training Loss: 1.0436\n",
      "Epoch [193/1000] - Validation Loss: 1.0839\n",
      "Epoch [194/1000] - Training Loss: 1.0436\n",
      "Epoch [194/1000] - Validation Loss: 1.0823\n",
      "Epoch [195/1000] - Training Loss: 1.0436\n",
      "Epoch [195/1000] - Validation Loss: 1.0819\n",
      "Epoch [196/1000] - Training Loss: 1.0436\n",
      "Epoch [196/1000] - Validation Loss: 1.0824\n",
      "Epoch [197/1000] - Training Loss: 1.0436\n",
      "Epoch [197/1000] - Validation Loss: 1.0833\n",
      "Epoch [198/1000] - Training Loss: 1.0436\n",
      "Epoch [198/1000] - Validation Loss: 1.0846\n",
      "Epoch [199/1000] - Training Loss: 1.0436\n",
      "Epoch [199/1000] - Validation Loss: 1.0848\n",
      "Epoch [200/1000] - Training Loss: 1.0436\n",
      "Epoch [200/1000] - Validation Loss: 1.0840\n",
      "Epoch [201/1000] - Training Loss: 1.0436\n",
      "Epoch [201/1000] - Validation Loss: 1.0861\n",
      "Epoch [202/1000] - Training Loss: 1.0436\n",
      "Epoch [202/1000] - Validation Loss: 1.0853\n",
      "Epoch [203/1000] - Training Loss: 1.0436\n",
      "Epoch [203/1000] - Validation Loss: 1.0842\n",
      "Epoch [204/1000] - Training Loss: 1.0436\n",
      "Epoch [204/1000] - Validation Loss: 1.0837\n",
      "Epoch [205/1000] - Training Loss: 1.0436\n",
      "Epoch [205/1000] - Validation Loss: 1.0853\n",
      "Epoch [206/1000] - Training Loss: 1.0436\n",
      "Epoch [206/1000] - Validation Loss: 1.0849\n",
      "Epoch [207/1000] - Training Loss: 1.0436\n",
      "Epoch [207/1000] - Validation Loss: 1.0840\n",
      "Epoch [208/1000] - Training Loss: 1.0436\n",
      "Epoch [208/1000] - Validation Loss: 1.0838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [209/1000] - Training Loss: 1.0436\n",
      "Epoch [209/1000] - Validation Loss: 1.0847\n",
      "Epoch [210/1000] - Training Loss: 1.0436\n",
      "Epoch [210/1000] - Validation Loss: 1.0860\n",
      "Epoch [211/1000] - Training Loss: 1.0436\n",
      "Epoch [211/1000] - Validation Loss: 1.0851\n",
      "Epoch [212/1000] - Training Loss: 1.0436\n",
      "Epoch [212/1000] - Validation Loss: 1.0832\n",
      "Epoch [213/1000] - Training Loss: 1.0436\n",
      "Epoch [213/1000] - Validation Loss: 1.0833\n",
      "Epoch [214/1000] - Training Loss: 1.0436\n",
      "Epoch [214/1000] - Validation Loss: 1.0842\n",
      "Epoch [215/1000] - Training Loss: 1.0436\n",
      "Epoch [215/1000] - Validation Loss: 1.0850\n",
      "Epoch [216/1000] - Training Loss: 1.0436\n",
      "Epoch [216/1000] - Validation Loss: 1.0849\n",
      "Epoch [217/1000] - Training Loss: 1.0436\n",
      "Epoch [217/1000] - Validation Loss: 1.0842\n",
      "Epoch [218/1000] - Training Loss: 1.0436\n",
      "Epoch [218/1000] - Validation Loss: 1.0841\n",
      "Epoch [219/1000] - Training Loss: 1.0436\n",
      "Epoch [219/1000] - Validation Loss: 1.0838\n",
      "Epoch [220/1000] - Training Loss: 1.0436\n",
      "Epoch [220/1000] - Validation Loss: 1.0839\n",
      "Epoch [221/1000] - Training Loss: 1.0436\n",
      "Epoch [221/1000] - Validation Loss: 1.0842\n",
      "Epoch [222/1000] - Training Loss: 1.0436\n",
      "Epoch [222/1000] - Validation Loss: 1.0841\n",
      "Epoch [223/1000] - Training Loss: 1.0436\n",
      "Epoch [223/1000] - Validation Loss: 1.0846\n",
      "Epoch [224/1000] - Training Loss: 1.0436\n",
      "Epoch [224/1000] - Validation Loss: 1.0859\n",
      "Epoch [225/1000] - Training Loss: 1.0436\n",
      "Epoch [225/1000] - Validation Loss: 1.0853\n",
      "Epoch [226/1000] - Training Loss: 1.0436\n",
      "Epoch [226/1000] - Validation Loss: 1.0858\n",
      "Epoch [227/1000] - Training Loss: 1.0436\n",
      "Epoch [227/1000] - Validation Loss: 1.0844\n",
      "Epoch [228/1000] - Training Loss: 1.0436\n",
      "Epoch [228/1000] - Validation Loss: 1.0833\n",
      "Epoch [229/1000] - Training Loss: 1.0436\n",
      "Epoch [229/1000] - Validation Loss: 1.0833\n",
      "Epoch [230/1000] - Training Loss: 1.0436\n",
      "Epoch [230/1000] - Validation Loss: 1.0825\n",
      "Epoch [231/1000] - Training Loss: 1.0436\n",
      "Epoch [231/1000] - Validation Loss: 1.0827\n",
      "Epoch [232/1000] - Training Loss: 1.0436\n",
      "Epoch [232/1000] - Validation Loss: 1.0829\n",
      "Epoch [233/1000] - Training Loss: 1.0436\n",
      "Epoch [233/1000] - Validation Loss: 1.0830\n",
      "Epoch [234/1000] - Training Loss: 1.0436\n",
      "Epoch [234/1000] - Validation Loss: 1.0830\n",
      "Epoch [235/1000] - Training Loss: 1.0436\n",
      "Epoch [235/1000] - Validation Loss: 1.0832\n",
      "Epoch [236/1000] - Training Loss: 1.0436\n",
      "Epoch [236/1000] - Validation Loss: 1.0834\n",
      "Epoch [237/1000] - Training Loss: 1.0436\n",
      "Epoch [237/1000] - Validation Loss: 1.0843\n",
      "Epoch [238/1000] - Training Loss: 1.0436\n",
      "Epoch [238/1000] - Validation Loss: 1.0834\n",
      "Epoch [239/1000] - Training Loss: 1.0436\n",
      "Epoch [239/1000] - Validation Loss: 1.0839\n",
      "Epoch [240/1000] - Training Loss: 1.0436\n",
      "Epoch [240/1000] - Validation Loss: 1.0841\n",
      "Epoch [241/1000] - Training Loss: 1.0436\n",
      "Epoch [241/1000] - Validation Loss: 1.0853\n",
      "Epoch [242/1000] - Training Loss: 1.0436\n",
      "Epoch [242/1000] - Validation Loss: 1.0843\n",
      "Epoch [243/1000] - Training Loss: 1.0436\n",
      "Epoch [243/1000] - Validation Loss: 1.0827\n",
      "Epoch [244/1000] - Training Loss: 1.0436\n",
      "Epoch [244/1000] - Validation Loss: 1.0825\n",
      "Epoch [245/1000] - Training Loss: 1.0436\n",
      "Epoch [245/1000] - Validation Loss: 1.0831\n",
      "Epoch [246/1000] - Training Loss: 1.0436\n",
      "Epoch [246/1000] - Validation Loss: 1.0839\n",
      "Epoch [247/1000] - Training Loss: 1.0436\n",
      "Epoch [247/1000] - Validation Loss: 1.0829\n",
      "Epoch [248/1000] - Training Loss: 1.0436\n",
      "Epoch [248/1000] - Validation Loss: 1.0828\n",
      "Epoch [249/1000] - Training Loss: 1.0436\n",
      "Epoch [249/1000] - Validation Loss: 1.0841\n",
      "Epoch [250/1000] - Training Loss: 1.0436\n",
      "Epoch [250/1000] - Validation Loss: 1.0836\n",
      "Epoch [251/1000] - Training Loss: 1.0436\n",
      "Epoch [251/1000] - Validation Loss: 1.0835\n",
      "Epoch [252/1000] - Training Loss: 1.0436\n",
      "Epoch [252/1000] - Validation Loss: 1.0837\n",
      "Epoch [253/1000] - Training Loss: 1.0436\n",
      "Epoch [253/1000] - Validation Loss: 1.0837\n",
      "Epoch [254/1000] - Training Loss: 1.0436\n",
      "Epoch [254/1000] - Validation Loss: 1.0841\n",
      "Epoch [255/1000] - Training Loss: 1.0436\n",
      "Epoch [255/1000] - Validation Loss: 1.0844\n",
      "Epoch [256/1000] - Training Loss: 1.0436\n",
      "Epoch [256/1000] - Validation Loss: 1.0853\n",
      "Epoch [257/1000] - Training Loss: 1.0436\n",
      "Epoch [257/1000] - Validation Loss: 1.0847\n",
      "Epoch [258/1000] - Training Loss: 1.0436\n",
      "Epoch [258/1000] - Validation Loss: 1.0839\n",
      "Epoch [259/1000] - Training Loss: 1.0436\n",
      "Epoch [259/1000] - Validation Loss: 1.0839\n",
      "Epoch [260/1000] - Training Loss: 1.0436\n",
      "Epoch [260/1000] - Validation Loss: 1.0839\n",
      "Epoch [261/1000] - Training Loss: 1.0436\n",
      "Epoch [261/1000] - Validation Loss: 1.0837\n",
      "Epoch [262/1000] - Training Loss: 1.0436\n",
      "Epoch [262/1000] - Validation Loss: 1.0833\n",
      "Epoch [263/1000] - Training Loss: 1.0436\n",
      "Epoch [263/1000] - Validation Loss: 1.0835\n",
      "Epoch [264/1000] - Training Loss: 1.0436\n",
      "Epoch [264/1000] - Validation Loss: 1.0831\n",
      "Epoch [265/1000] - Training Loss: 1.0436\n",
      "Epoch [265/1000] - Validation Loss: 1.0843\n",
      "Epoch [266/1000] - Training Loss: 1.0436\n",
      "Epoch [266/1000] - Validation Loss: 1.0835\n",
      "Epoch [267/1000] - Training Loss: 1.0436\n",
      "Epoch [267/1000] - Validation Loss: 1.0837\n",
      "Epoch [268/1000] - Training Loss: 1.0436\n",
      "Epoch [268/1000] - Validation Loss: 1.0848\n",
      "Epoch [269/1000] - Training Loss: 1.0436\n",
      "Epoch [269/1000] - Validation Loss: 1.0841\n",
      "Epoch [270/1000] - Training Loss: 1.0436\n",
      "Epoch [270/1000] - Validation Loss: 1.0848\n",
      "Epoch [271/1000] - Training Loss: 1.0436\n",
      "Epoch [271/1000] - Validation Loss: 1.0846\n",
      "Epoch [272/1000] - Training Loss: 1.0436\n",
      "Epoch [272/1000] - Validation Loss: 1.0837\n",
      "Epoch [273/1000] - Training Loss: 1.0436\n",
      "Epoch [273/1000] - Validation Loss: 1.0835\n",
      "Epoch [274/1000] - Training Loss: 1.0436\n",
      "Epoch [274/1000] - Validation Loss: 1.0836\n",
      "Epoch [275/1000] - Training Loss: 1.0436\n",
      "Epoch [275/1000] - Validation Loss: 1.0831\n",
      "Epoch [276/1000] - Training Loss: 1.0436\n",
      "Epoch [276/1000] - Validation Loss: 1.0831\n",
      "Epoch [277/1000] - Training Loss: 1.0436\n",
      "Epoch [277/1000] - Validation Loss: 1.0837\n",
      "Epoch [278/1000] - Training Loss: 1.0436\n",
      "Epoch [278/1000] - Validation Loss: 1.0835\n",
      "Epoch [279/1000] - Training Loss: 1.0436\n",
      "Epoch [279/1000] - Validation Loss: 1.0832\n",
      "Epoch [280/1000] - Training Loss: 1.0436\n",
      "Epoch [280/1000] - Validation Loss: 1.0836\n",
      "Epoch [281/1000] - Training Loss: 1.0436\n",
      "Epoch [281/1000] - Validation Loss: 1.0828\n",
      "Epoch [282/1000] - Training Loss: 1.0436\n",
      "Epoch [282/1000] - Validation Loss: 1.0832\n",
      "Epoch [283/1000] - Training Loss: 1.0436\n",
      "Epoch [283/1000] - Validation Loss: 1.0835\n",
      "Epoch [284/1000] - Training Loss: 1.0436\n",
      "Epoch [284/1000] - Validation Loss: 1.0838\n",
      "Epoch [285/1000] - Training Loss: 1.0436\n",
      "Epoch [285/1000] - Validation Loss: 1.0824\n",
      "Epoch [286/1000] - Training Loss: 1.0436\n",
      "Epoch [286/1000] - Validation Loss: 1.0823\n",
      "Epoch [287/1000] - Training Loss: 1.0436\n",
      "Epoch [287/1000] - Validation Loss: 1.0830\n",
      "Epoch [288/1000] - Training Loss: 1.0436\n",
      "Epoch [288/1000] - Validation Loss: 1.0830\n",
      "Epoch [289/1000] - Training Loss: 1.0436\n",
      "Epoch [289/1000] - Validation Loss: 1.0832\n",
      "Epoch [290/1000] - Training Loss: 1.0436\n",
      "Epoch [290/1000] - Validation Loss: 1.0838\n",
      "Epoch [291/1000] - Training Loss: 1.0436\n",
      "Epoch [291/1000] - Validation Loss: 1.0840\n",
      "Epoch [292/1000] - Training Loss: 1.0436\n",
      "Epoch [292/1000] - Validation Loss: 1.0837\n",
      "Epoch [293/1000] - Training Loss: 1.0436\n",
      "Epoch [293/1000] - Validation Loss: 1.0842\n",
      "Epoch [294/1000] - Training Loss: 1.0436\n",
      "Epoch [294/1000] - Validation Loss: 1.0842\n",
      "Epoch [295/1000] - Training Loss: 1.0436\n",
      "Epoch [295/1000] - Validation Loss: 1.0843\n",
      "Epoch [296/1000] - Training Loss: 1.0436\n",
      "Epoch [296/1000] - Validation Loss: 1.0841\n",
      "Epoch [297/1000] - Training Loss: 1.0436\n",
      "Epoch [297/1000] - Validation Loss: 1.0834\n",
      "Epoch [298/1000] - Training Loss: 1.0436\n",
      "Epoch [298/1000] - Validation Loss: 1.0827\n",
      "Epoch [299/1000] - Training Loss: 1.0436\n",
      "Epoch [299/1000] - Validation Loss: 1.0831\n",
      "Epoch [300/1000] - Training Loss: 1.0436\n",
      "Epoch [300/1000] - Validation Loss: 1.0831\n",
      "Epoch [301/1000] - Training Loss: 1.0436\n",
      "Epoch [301/1000] - Validation Loss: 1.0827\n",
      "Epoch [302/1000] - Training Loss: 1.0436\n",
      "Epoch [302/1000] - Validation Loss: 1.0826\n",
      "Epoch [303/1000] - Training Loss: 1.0436\n",
      "Epoch [303/1000] - Validation Loss: 1.0842\n",
      "Epoch [304/1000] - Training Loss: 1.0436\n",
      "Epoch [304/1000] - Validation Loss: 1.0900\n",
      "Epoch [305/1000] - Training Loss: 1.0436\n",
      "Epoch [305/1000] - Validation Loss: 1.0866\n",
      "Epoch [306/1000] - Training Loss: 1.0436\n",
      "Epoch [306/1000] - Validation Loss: 1.0857\n",
      "Epoch [307/1000] - Training Loss: 1.0436\n",
      "Epoch [307/1000] - Validation Loss: 1.0849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [308/1000] - Training Loss: 1.0436\n",
      "Epoch [308/1000] - Validation Loss: 1.0856\n",
      "Epoch [309/1000] - Training Loss: 1.0436\n",
      "Epoch [309/1000] - Validation Loss: 1.0854\n",
      "Epoch [310/1000] - Training Loss: 1.0436\n",
      "Epoch [310/1000] - Validation Loss: 1.0848\n",
      "Epoch [311/1000] - Training Loss: 1.0436\n",
      "Epoch [311/1000] - Validation Loss: 1.0840\n",
      "Epoch [312/1000] - Training Loss: 1.0436\n",
      "Epoch [312/1000] - Validation Loss: 1.0849\n",
      "Epoch [313/1000] - Training Loss: 1.0436\n",
      "Epoch [313/1000] - Validation Loss: 1.0831\n",
      "Epoch [314/1000] - Training Loss: 1.0436\n",
      "Epoch [314/1000] - Validation Loss: 1.0831\n",
      "Epoch [315/1000] - Training Loss: 1.0436\n",
      "Epoch [315/1000] - Validation Loss: 1.0834\n",
      "Epoch [316/1000] - Training Loss: 1.0436\n",
      "Epoch [316/1000] - Validation Loss: 1.0839\n",
      "Epoch [317/1000] - Training Loss: 1.0436\n",
      "Epoch [317/1000] - Validation Loss: 1.0828\n",
      "Epoch [318/1000] - Training Loss: 1.0436\n",
      "Epoch [318/1000] - Validation Loss: 1.0828\n",
      "Epoch [319/1000] - Training Loss: 1.0436\n",
      "Epoch [319/1000] - Validation Loss: 1.0826\n",
      "Epoch [320/1000] - Training Loss: 1.0436\n",
      "Epoch [320/1000] - Validation Loss: 1.0827\n",
      "Epoch [321/1000] - Training Loss: 1.0436\n",
      "Epoch [321/1000] - Validation Loss: 1.0832\n",
      "Epoch [322/1000] - Training Loss: 1.0436\n",
      "Epoch [322/1000] - Validation Loss: 1.0830\n",
      "Epoch [323/1000] - Training Loss: 1.0436\n",
      "Epoch [323/1000] - Validation Loss: 1.0825\n",
      "Epoch [324/1000] - Training Loss: 1.0436\n",
      "Epoch [324/1000] - Validation Loss: 1.0830\n",
      "Epoch [325/1000] - Training Loss: 1.0436\n",
      "Epoch [325/1000] - Validation Loss: 1.0827\n",
      "Epoch [326/1000] - Training Loss: 1.0436\n",
      "Epoch [326/1000] - Validation Loss: 1.0822\n",
      "Epoch [327/1000] - Training Loss: 1.0436\n",
      "Epoch [327/1000] - Validation Loss: 1.0828\n",
      "Epoch [328/1000] - Training Loss: 1.0436\n",
      "Epoch [328/1000] - Validation Loss: 1.0844\n",
      "Epoch [329/1000] - Training Loss: 1.0436\n",
      "Epoch [329/1000] - Validation Loss: 1.0835\n",
      "Epoch [330/1000] - Training Loss: 1.0436\n",
      "Epoch [330/1000] - Validation Loss: 1.0837\n",
      "Epoch [331/1000] - Training Loss: 1.0436\n",
      "Epoch [331/1000] - Validation Loss: 1.0837\n",
      "Epoch [332/1000] - Training Loss: 1.0436\n",
      "Epoch [332/1000] - Validation Loss: 1.0842\n",
      "Epoch [333/1000] - Training Loss: 1.0436\n",
      "Epoch [333/1000] - Validation Loss: 1.0838\n",
      "Epoch [334/1000] - Training Loss: 1.0436\n",
      "Epoch [334/1000] - Validation Loss: 1.0837\n",
      "Epoch [335/1000] - Training Loss: 1.0436\n",
      "Epoch [335/1000] - Validation Loss: 1.0840\n",
      "Epoch [336/1000] - Training Loss: 1.0436\n",
      "Epoch [336/1000] - Validation Loss: 1.0843\n",
      "Epoch [337/1000] - Training Loss: 1.0436\n",
      "Epoch [337/1000] - Validation Loss: 1.0854\n",
      "Epoch [338/1000] - Training Loss: 1.0436\n",
      "Epoch [338/1000] - Validation Loss: 1.0832\n",
      "Epoch [339/1000] - Training Loss: 1.0436\n",
      "Epoch [339/1000] - Validation Loss: 1.0829\n",
      "Epoch [340/1000] - Training Loss: 1.0436\n",
      "Epoch [340/1000] - Validation Loss: 1.0829\n",
      "Epoch [341/1000] - Training Loss: 1.0436\n",
      "Epoch [341/1000] - Validation Loss: 1.0828\n",
      "Epoch [342/1000] - Training Loss: 1.0436\n",
      "Epoch [342/1000] - Validation Loss: 1.0834\n",
      "Epoch [343/1000] - Training Loss: 1.0436\n",
      "Epoch [343/1000] - Validation Loss: 1.0832\n",
      "Epoch [344/1000] - Training Loss: 1.0436\n",
      "Epoch [344/1000] - Validation Loss: 1.0834\n",
      "Epoch [345/1000] - Training Loss: 1.0436\n",
      "Epoch [345/1000] - Validation Loss: 1.0829\n",
      "Epoch [346/1000] - Training Loss: 1.0436\n",
      "Epoch [346/1000] - Validation Loss: 1.0831\n",
      "Epoch [347/1000] - Training Loss: 1.0436\n",
      "Epoch [347/1000] - Validation Loss: 1.0827\n",
      "Epoch [348/1000] - Training Loss: 1.0436\n",
      "Epoch [348/1000] - Validation Loss: 1.0824\n",
      "Epoch [349/1000] - Training Loss: 1.0436\n",
      "Epoch [349/1000] - Validation Loss: 1.0825\n",
      "Epoch [350/1000] - Training Loss: 1.0436\n",
      "Epoch [350/1000] - Validation Loss: 1.0826\n",
      "Epoch [351/1000] - Training Loss: 1.0436\n",
      "Epoch [351/1000] - Validation Loss: 1.0823\n",
      "Epoch [352/1000] - Training Loss: 1.0436\n",
      "Epoch [352/1000] - Validation Loss: 1.0825\n",
      "Epoch [353/1000] - Training Loss: 1.0436\n",
      "Epoch [353/1000] - Validation Loss: 1.0828\n",
      "Epoch [354/1000] - Training Loss: 1.0436\n",
      "Epoch [354/1000] - Validation Loss: 1.0827\n",
      "Epoch [355/1000] - Training Loss: 1.0436\n",
      "Epoch [355/1000] - Validation Loss: 1.0825\n",
      "Epoch [356/1000] - Training Loss: 1.0436\n",
      "Epoch [356/1000] - Validation Loss: 1.0829\n",
      "Epoch [357/1000] - Training Loss: 1.0436\n",
      "Epoch [357/1000] - Validation Loss: 1.0837\n",
      "Epoch [358/1000] - Training Loss: 1.0436\n",
      "Epoch [358/1000] - Validation Loss: 1.0842\n",
      "Epoch [359/1000] - Training Loss: 1.0436\n",
      "Epoch [359/1000] - Validation Loss: 1.0841\n",
      "Epoch [360/1000] - Training Loss: 1.0436\n",
      "Epoch [360/1000] - Validation Loss: 1.0836\n",
      "Epoch [361/1000] - Training Loss: 1.0436\n",
      "Epoch [361/1000] - Validation Loss: 1.0841\n",
      "Epoch [362/1000] - Training Loss: 1.0436\n",
      "Epoch [362/1000] - Validation Loss: 1.0846\n",
      "Epoch [363/1000] - Training Loss: 1.0436\n",
      "Epoch [363/1000] - Validation Loss: 1.0838\n",
      "Epoch [364/1000] - Training Loss: 1.0436\n",
      "Epoch [364/1000] - Validation Loss: 1.0840\n",
      "Epoch [365/1000] - Training Loss: 1.0436\n",
      "Epoch [365/1000] - Validation Loss: 1.0834\n",
      "Epoch [366/1000] - Training Loss: 1.0436\n",
      "Epoch [366/1000] - Validation Loss: 1.0833\n",
      "Epoch [367/1000] - Training Loss: 1.0436\n",
      "Epoch [367/1000] - Validation Loss: 1.0841\n",
      "Epoch [368/1000] - Training Loss: 1.0436\n",
      "Epoch [368/1000] - Validation Loss: 1.0844\n",
      "Epoch [369/1000] - Training Loss: 1.0436\n",
      "Epoch [369/1000] - Validation Loss: 1.0834\n",
      "Epoch [370/1000] - Training Loss: 1.0436\n",
      "Epoch [370/1000] - Validation Loss: 1.0840\n",
      "Epoch [371/1000] - Training Loss: 1.0436\n",
      "Epoch [371/1000] - Validation Loss: 1.0841\n",
      "Epoch [372/1000] - Training Loss: 1.0436\n",
      "Epoch [372/1000] - Validation Loss: 1.0847\n",
      "Epoch [373/1000] - Training Loss: 1.0436\n",
      "Epoch [373/1000] - Validation Loss: 1.0842\n",
      "Epoch [374/1000] - Training Loss: 1.0436\n",
      "Epoch [374/1000] - Validation Loss: 1.0830\n",
      "Epoch [375/1000] - Training Loss: 1.0436\n",
      "Epoch [375/1000] - Validation Loss: 1.0840\n",
      "Epoch [376/1000] - Training Loss: 1.0436\n",
      "Epoch [376/1000] - Validation Loss: 1.0848\n",
      "Epoch [377/1000] - Training Loss: 1.0436\n",
      "Epoch [377/1000] - Validation Loss: 1.0838\n",
      "Epoch [378/1000] - Training Loss: 1.0436\n",
      "Epoch [378/1000] - Validation Loss: 1.0816\n",
      "Epoch [379/1000] - Training Loss: 1.0436\n",
      "Epoch [379/1000] - Validation Loss: 1.0829\n",
      "Epoch [380/1000] - Training Loss: 1.0436\n",
      "Epoch [380/1000] - Validation Loss: 1.0833\n",
      "Epoch [381/1000] - Training Loss: 1.0436\n",
      "Epoch [381/1000] - Validation Loss: 1.0825\n",
      "Epoch [382/1000] - Training Loss: 1.0436\n",
      "Epoch [382/1000] - Validation Loss: 1.0824\n",
      "Epoch [383/1000] - Training Loss: 1.0436\n",
      "Epoch [383/1000] - Validation Loss: 1.0827\n",
      "Epoch [384/1000] - Training Loss: 1.0436\n",
      "Epoch [384/1000] - Validation Loss: 1.0830\n",
      "Epoch [385/1000] - Training Loss: 1.0436\n",
      "Epoch [385/1000] - Validation Loss: 1.0827\n",
      "Epoch [386/1000] - Training Loss: 1.0436\n",
      "Epoch [386/1000] - Validation Loss: 1.0824\n",
      "Epoch [387/1000] - Training Loss: 1.0436\n",
      "Epoch [387/1000] - Validation Loss: 1.0815\n",
      "Epoch [388/1000] - Training Loss: 1.0436\n",
      "Epoch [388/1000] - Validation Loss: 1.0820\n",
      "Epoch [389/1000] - Training Loss: 1.0436\n",
      "Epoch [389/1000] - Validation Loss: 1.0823\n",
      "Epoch [390/1000] - Training Loss: 1.0436\n",
      "Epoch [390/1000] - Validation Loss: 1.0829\n",
      "Epoch [391/1000] - Training Loss: 1.0436\n",
      "Epoch [391/1000] - Validation Loss: 1.0852\n",
      "Epoch [392/1000] - Training Loss: 1.0436\n",
      "Epoch [392/1000] - Validation Loss: 1.0831\n",
      "Epoch [393/1000] - Training Loss: 1.0436\n",
      "Epoch [393/1000] - Validation Loss: 1.0833\n",
      "Epoch [394/1000] - Training Loss: 1.0436\n",
      "Epoch [394/1000] - Validation Loss: 1.0834\n",
      "Epoch [395/1000] - Training Loss: 1.0436\n",
      "Epoch [395/1000] - Validation Loss: 1.0831\n",
      "Epoch [396/1000] - Training Loss: 1.0436\n",
      "Epoch [396/1000] - Validation Loss: 1.0818\n",
      "Epoch [397/1000] - Training Loss: 1.0436\n",
      "Epoch [397/1000] - Validation Loss: 1.0817\n",
      "Epoch [398/1000] - Training Loss: 1.0436\n",
      "Epoch [398/1000] - Validation Loss: 1.0832\n",
      "Epoch [399/1000] - Training Loss: 1.0436\n",
      "Epoch [399/1000] - Validation Loss: 1.0831\n",
      "Epoch [400/1000] - Training Loss: 1.0436\n",
      "Epoch [400/1000] - Validation Loss: 1.0831\n",
      "Epoch [401/1000] - Training Loss: 1.0436\n",
      "Epoch [401/1000] - Validation Loss: 1.0826\n",
      "Epoch [402/1000] - Training Loss: 1.0436\n",
      "Epoch [402/1000] - Validation Loss: 1.0814\n",
      "Epoch [403/1000] - Training Loss: 1.0436\n",
      "Epoch [403/1000] - Validation Loss: 1.0819\n",
      "Epoch [404/1000] - Training Loss: 1.0436\n",
      "Epoch [404/1000] - Validation Loss: 1.0822\n",
      "Epoch [405/1000] - Training Loss: 1.0436\n",
      "Epoch [405/1000] - Validation Loss: 1.0820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [406/1000] - Training Loss: 1.0436\n",
      "Epoch [406/1000] - Validation Loss: 1.0826\n",
      "Epoch [407/1000] - Training Loss: 1.0436\n",
      "Epoch [407/1000] - Validation Loss: 1.0808\n",
      "Epoch [408/1000] - Training Loss: 1.0436\n",
      "Epoch [408/1000] - Validation Loss: 1.0800\n",
      "Epoch [409/1000] - Training Loss: 1.0436\n",
      "Epoch [409/1000] - Validation Loss: 1.0848\n",
      "Epoch [410/1000] - Training Loss: 1.0436\n",
      "Epoch [410/1000] - Validation Loss: 1.0846\n",
      "Epoch [411/1000] - Training Loss: 1.0436\n",
      "Epoch [411/1000] - Validation Loss: 1.0839\n",
      "Epoch [412/1000] - Training Loss: 1.0436\n",
      "Epoch [412/1000] - Validation Loss: 1.0827\n",
      "Epoch [413/1000] - Training Loss: 1.0436\n",
      "Epoch [413/1000] - Validation Loss: 1.0827\n",
      "Epoch [414/1000] - Training Loss: 1.0436\n",
      "Epoch [414/1000] - Validation Loss: 1.0845\n",
      "Epoch [415/1000] - Training Loss: 1.0436\n",
      "Epoch [415/1000] - Validation Loss: 1.0848\n",
      "Epoch [416/1000] - Training Loss: 1.0436\n",
      "Epoch [416/1000] - Validation Loss: 1.0816\n",
      "Epoch [417/1000] - Training Loss: 1.0436\n",
      "Epoch [417/1000] - Validation Loss: 1.0817\n",
      "Epoch [418/1000] - Training Loss: 1.0436\n",
      "Epoch [418/1000] - Validation Loss: 1.0817\n",
      "Epoch [419/1000] - Training Loss: 1.0436\n",
      "Epoch [419/1000] - Validation Loss: 1.0840\n",
      "Epoch [420/1000] - Training Loss: 1.0436\n",
      "Epoch [420/1000] - Validation Loss: 1.0844\n",
      "Epoch [421/1000] - Training Loss: 1.0436\n",
      "Epoch [421/1000] - Validation Loss: 1.0840\n",
      "Epoch [422/1000] - Training Loss: 1.0436\n",
      "Epoch [422/1000] - Validation Loss: 1.0837\n",
      "Epoch [423/1000] - Training Loss: 1.0436\n",
      "Epoch [423/1000] - Validation Loss: 1.0839\n",
      "Epoch [424/1000] - Training Loss: 1.0436\n",
      "Epoch [424/1000] - Validation Loss: 1.0836\n",
      "Epoch [425/1000] - Training Loss: 1.0436\n",
      "Epoch [425/1000] - Validation Loss: 1.0844\n",
      "Epoch [426/1000] - Training Loss: 1.0436\n",
      "Epoch [426/1000] - Validation Loss: 1.0852\n",
      "Epoch [427/1000] - Training Loss: 1.0436\n",
      "Epoch [427/1000] - Validation Loss: 1.0860\n",
      "Epoch [428/1000] - Training Loss: 1.0436\n",
      "Epoch [428/1000] - Validation Loss: 1.0850\n",
      "Epoch [429/1000] - Training Loss: 1.0436\n",
      "Epoch [429/1000] - Validation Loss: 1.0841\n",
      "Epoch [430/1000] - Training Loss: 1.0436\n",
      "Epoch [430/1000] - Validation Loss: 1.0848\n",
      "Epoch [431/1000] - Training Loss: 1.0436\n",
      "Epoch [431/1000] - Validation Loss: 1.0839\n",
      "Epoch [432/1000] - Training Loss: 1.0436\n",
      "Epoch [432/1000] - Validation Loss: 1.0803\n",
      "Epoch [433/1000] - Training Loss: 1.0436\n",
      "Epoch [433/1000] - Validation Loss: 1.0818\n",
      "Epoch [434/1000] - Training Loss: 1.0436\n",
      "Epoch [434/1000] - Validation Loss: 1.0842\n",
      "Epoch [435/1000] - Training Loss: 1.0436\n",
      "Epoch [435/1000] - Validation Loss: 1.0843\n",
      "Epoch [436/1000] - Training Loss: 1.0436\n",
      "Epoch [436/1000] - Validation Loss: 1.0842\n",
      "Epoch [437/1000] - Training Loss: 1.0436\n",
      "Epoch [437/1000] - Validation Loss: 1.0829\n",
      "Epoch [438/1000] - Training Loss: 1.0436\n",
      "Epoch [438/1000] - Validation Loss: 1.0830\n",
      "Epoch [439/1000] - Training Loss: 1.0436\n",
      "Epoch [439/1000] - Validation Loss: 1.0846\n",
      "Epoch [440/1000] - Training Loss: 1.0436\n",
      "Epoch [440/1000] - Validation Loss: 1.0828\n",
      "Epoch [441/1000] - Training Loss: 1.0436\n",
      "Epoch [441/1000] - Validation Loss: 1.0811\n",
      "Epoch [442/1000] - Training Loss: 1.0436\n",
      "Epoch [442/1000] - Validation Loss: 1.0802\n",
      "Epoch [443/1000] - Training Loss: 1.0436\n",
      "Epoch [443/1000] - Validation Loss: 1.0829\n",
      "Epoch [444/1000] - Training Loss: 1.0436\n",
      "Epoch [444/1000] - Validation Loss: 1.0885\n",
      "Epoch [445/1000] - Training Loss: 1.0436\n",
      "Epoch [445/1000] - Validation Loss: 1.0880\n",
      "Epoch [446/1000] - Training Loss: 1.0436\n",
      "Epoch [446/1000] - Validation Loss: 1.0976\n",
      "Epoch [447/1000] - Training Loss: 1.0436\n",
      "Epoch [447/1000] - Validation Loss: 1.0809\n",
      "Epoch [448/1000] - Training Loss: 1.0436\n",
      "Epoch [448/1000] - Validation Loss: 1.0919\n",
      "Epoch [449/1000] - Training Loss: 1.0436\n",
      "Epoch [449/1000] - Validation Loss: 1.0867\n",
      "Epoch [450/1000] - Training Loss: 1.0436\n",
      "Epoch [450/1000] - Validation Loss: 1.0847\n",
      "Epoch [451/1000] - Training Loss: 1.0436\n",
      "Epoch [451/1000] - Validation Loss: 1.0865\n",
      "Epoch [452/1000] - Training Loss: 1.0436\n",
      "Epoch [452/1000] - Validation Loss: 1.0870\n",
      "Epoch [453/1000] - Training Loss: 1.0436\n",
      "Epoch [453/1000] - Validation Loss: 1.0871\n",
      "Epoch [454/1000] - Training Loss: 1.0436\n",
      "Epoch [454/1000] - Validation Loss: 1.0870\n",
      "Epoch [455/1000] - Training Loss: 1.0436\n",
      "Epoch [455/1000] - Validation Loss: 1.0870\n",
      "Epoch [456/1000] - Training Loss: 1.0436\n",
      "Epoch [456/1000] - Validation Loss: 1.0870\n",
      "Epoch [457/1000] - Training Loss: 1.0436\n",
      "Epoch [457/1000] - Validation Loss: 1.0866\n",
      "Epoch [458/1000] - Training Loss: 1.0436\n",
      "Epoch [458/1000] - Validation Loss: 1.0866\n",
      "Epoch [459/1000] - Training Loss: 1.0436\n",
      "Epoch [459/1000] - Validation Loss: 1.0868\n",
      "Epoch [460/1000] - Training Loss: 1.0436\n",
      "Epoch [460/1000] - Validation Loss: 1.0869\n",
      "Epoch [461/1000] - Training Loss: 1.0436\n",
      "Epoch [461/1000] - Validation Loss: 1.0871\n",
      "Epoch [462/1000] - Training Loss: 1.0436\n",
      "Epoch [462/1000] - Validation Loss: 1.0871\n",
      "Epoch [463/1000] - Training Loss: 1.0436\n",
      "Epoch [463/1000] - Validation Loss: 1.0869\n",
      "Epoch [464/1000] - Training Loss: 1.0436\n",
      "Epoch [464/1000] - Validation Loss: 1.0868\n",
      "Epoch [465/1000] - Training Loss: 1.0436\n",
      "Epoch [465/1000] - Validation Loss: 1.0866\n",
      "Epoch [466/1000] - Training Loss: 1.0436\n",
      "Epoch [466/1000] - Validation Loss: 1.0867\n",
      "Epoch [467/1000] - Training Loss: 1.0436\n",
      "Epoch [467/1000] - Validation Loss: 1.0868\n",
      "Epoch [468/1000] - Training Loss: 1.0436\n",
      "Epoch [468/1000] - Validation Loss: 1.0869\n",
      "Epoch [469/1000] - Training Loss: 1.0436\n",
      "Epoch [469/1000] - Validation Loss: 1.0875\n",
      "Epoch [470/1000] - Training Loss: 1.0436\n",
      "Epoch [470/1000] - Validation Loss: 1.0875\n",
      "Epoch [471/1000] - Training Loss: 1.0436\n",
      "Epoch [471/1000] - Validation Loss: 1.0876\n",
      "Epoch [472/1000] - Training Loss: 1.0436\n",
      "Epoch [472/1000] - Validation Loss: 1.0875\n",
      "Epoch [473/1000] - Training Loss: 1.0436\n",
      "Epoch [473/1000] - Validation Loss: 1.0874\n",
      "Epoch [474/1000] - Training Loss: 1.0436\n",
      "Epoch [474/1000] - Validation Loss: 1.0872\n",
      "Epoch [475/1000] - Training Loss: 1.0436\n",
      "Epoch [475/1000] - Validation Loss: 1.0871\n",
      "Epoch [476/1000] - Training Loss: 1.0436\n",
      "Epoch [476/1000] - Validation Loss: 1.0846\n",
      "Epoch [477/1000] - Training Loss: 1.0436\n",
      "Epoch [477/1000] - Validation Loss: 1.0837\n",
      "Epoch [478/1000] - Training Loss: 1.0436\n",
      "Epoch [478/1000] - Validation Loss: 1.0845\n",
      "Epoch [479/1000] - Training Loss: 1.0436\n",
      "Epoch [479/1000] - Validation Loss: 1.0853\n",
      "Epoch [480/1000] - Training Loss: 1.0436\n",
      "Epoch [480/1000] - Validation Loss: 1.0856\n",
      "Epoch [481/1000] - Training Loss: 1.0436\n",
      "Epoch [481/1000] - Validation Loss: 1.0858\n",
      "Epoch [482/1000] - Training Loss: 1.0436\n",
      "Epoch [482/1000] - Validation Loss: 1.0863\n",
      "Epoch [483/1000] - Training Loss: 1.0436\n",
      "Epoch [483/1000] - Validation Loss: 1.0866\n",
      "Epoch [484/1000] - Training Loss: 1.0436\n",
      "Epoch [484/1000] - Validation Loss: 1.0866\n",
      "Epoch [485/1000] - Training Loss: 1.0436\n",
      "Epoch [485/1000] - Validation Loss: 1.0864\n",
      "Epoch [486/1000] - Training Loss: 1.0436\n",
      "Epoch [486/1000] - Validation Loss: 1.0865\n",
      "Epoch [487/1000] - Training Loss: 1.0436\n",
      "Epoch [487/1000] - Validation Loss: 1.0866\n",
      "Epoch [488/1000] - Training Loss: 1.0436\n",
      "Epoch [488/1000] - Validation Loss: 1.0868\n",
      "Epoch [489/1000] - Training Loss: 1.0436\n",
      "Epoch [489/1000] - Validation Loss: 1.0874\n",
      "Epoch [490/1000] - Training Loss: 1.0436\n",
      "Epoch [490/1000] - Validation Loss: 1.0880\n",
      "Epoch [491/1000] - Training Loss: 1.0436\n",
      "Epoch [491/1000] - Validation Loss: 1.0875\n",
      "Epoch [492/1000] - Training Loss: 1.0436\n",
      "Epoch [492/1000] - Validation Loss: 1.0874\n",
      "Epoch [493/1000] - Training Loss: 1.0436\n",
      "Epoch [493/1000] - Validation Loss: 1.0871\n",
      "Epoch [494/1000] - Training Loss: 1.0436\n",
      "Epoch [494/1000] - Validation Loss: 1.0870\n",
      "Epoch [495/1000] - Training Loss: 1.0436\n",
      "Epoch [495/1000] - Validation Loss: 1.0870\n",
      "Epoch [496/1000] - Training Loss: 1.0436\n",
      "Epoch [496/1000] - Validation Loss: 1.0870\n",
      "Epoch [497/1000] - Training Loss: 1.0436\n",
      "Epoch [497/1000] - Validation Loss: 1.0871\n",
      "Epoch [498/1000] - Training Loss: 1.0436\n",
      "Epoch [498/1000] - Validation Loss: 1.0871\n",
      "Epoch [499/1000] - Training Loss: 1.0436\n",
      "Epoch [499/1000] - Validation Loss: 1.0871\n",
      "Epoch [500/1000] - Training Loss: 1.0436\n",
      "Epoch [500/1000] - Validation Loss: 1.0870\n",
      "Epoch [501/1000] - Training Loss: 1.0436\n",
      "Epoch [501/1000] - Validation Loss: 1.0869\n",
      "Epoch [502/1000] - Training Loss: 1.0436\n",
      "Epoch [502/1000] - Validation Loss: 1.0869\n",
      "Epoch [503/1000] - Training Loss: 1.0436\n",
      "Epoch [503/1000] - Validation Loss: 1.0871\n",
      "Epoch [504/1000] - Training Loss: 1.0436\n",
      "Epoch [504/1000] - Validation Loss: 1.0871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [505/1000] - Training Loss: 1.0436\n",
      "Epoch [505/1000] - Validation Loss: 1.0871\n",
      "Epoch [506/1000] - Training Loss: 1.0436\n",
      "Epoch [506/1000] - Validation Loss: 1.0870\n",
      "Epoch [507/1000] - Training Loss: 1.0436\n",
      "Epoch [507/1000] - Validation Loss: 1.0873\n",
      "Epoch [508/1000] - Training Loss: 1.0436\n",
      "Epoch [508/1000] - Validation Loss: 1.0869\n",
      "Epoch [509/1000] - Training Loss: 1.0436\n",
      "Epoch [509/1000] - Validation Loss: 1.0869\n",
      "Epoch [510/1000] - Training Loss: 1.0436\n",
      "Epoch [510/1000] - Validation Loss: 1.0868\n",
      "Epoch [511/1000] - Training Loss: 1.0436\n",
      "Epoch [511/1000] - Validation Loss: 1.0868\n",
      "Epoch [512/1000] - Training Loss: 1.0436\n",
      "Epoch [512/1000] - Validation Loss: 1.0866\n",
      "Epoch [513/1000] - Training Loss: 1.0436\n",
      "Epoch [513/1000] - Validation Loss: 1.0861\n",
      "Epoch [514/1000] - Training Loss: 1.0436\n",
      "Epoch [514/1000] - Validation Loss: 1.0860\n",
      "Epoch [515/1000] - Training Loss: 1.0436\n",
      "Epoch [515/1000] - Validation Loss: 1.0861\n",
      "Epoch [516/1000] - Training Loss: 1.0436\n",
      "Epoch [516/1000] - Validation Loss: 1.0859\n",
      "Epoch [517/1000] - Training Loss: 1.0436\n",
      "Epoch [517/1000] - Validation Loss: 1.0858\n",
      "Epoch [518/1000] - Training Loss: 1.0436\n",
      "Epoch [518/1000] - Validation Loss: 1.0860\n",
      "Epoch [519/1000] - Training Loss: 1.0436\n",
      "Epoch [519/1000] - Validation Loss: 1.0861\n",
      "Epoch [520/1000] - Training Loss: 1.0436\n",
      "Epoch [520/1000] - Validation Loss: 1.0862\n",
      "Epoch [521/1000] - Training Loss: 1.0436\n",
      "Epoch [521/1000] - Validation Loss: 1.0862\n",
      "Epoch [522/1000] - Training Loss: 1.0436\n",
      "Epoch [522/1000] - Validation Loss: 1.0862\n",
      "Epoch [523/1000] - Training Loss: 1.0436\n",
      "Epoch [523/1000] - Validation Loss: 1.0860\n",
      "Epoch [524/1000] - Training Loss: 1.0436\n",
      "Epoch [524/1000] - Validation Loss: 1.0849\n",
      "Epoch [525/1000] - Training Loss: 1.0436\n",
      "Epoch [525/1000] - Validation Loss: 1.0849\n",
      "Epoch [526/1000] - Training Loss: 1.0436\n",
      "Epoch [526/1000] - Validation Loss: 1.0830\n",
      "Epoch [527/1000] - Training Loss: 1.0436\n",
      "Epoch [527/1000] - Validation Loss: 1.0842\n",
      "Epoch [528/1000] - Training Loss: 1.0436\n",
      "Epoch [528/1000] - Validation Loss: 1.0854\n",
      "Epoch [529/1000] - Training Loss: 1.0436\n",
      "Epoch [529/1000] - Validation Loss: 1.0855\n",
      "Epoch [530/1000] - Training Loss: 1.0436\n",
      "Epoch [530/1000] - Validation Loss: 1.0856\n",
      "Epoch [531/1000] - Training Loss: 1.0436\n",
      "Epoch [531/1000] - Validation Loss: 1.0854\n",
      "Epoch [532/1000] - Training Loss: 1.0436\n",
      "Epoch [532/1000] - Validation Loss: 1.0859\n",
      "Epoch [533/1000] - Training Loss: 1.0436\n",
      "Epoch [533/1000] - Validation Loss: 1.0862\n",
      "Epoch [534/1000] - Training Loss: 1.0436\n",
      "Epoch [534/1000] - Validation Loss: 1.0864\n",
      "Epoch [535/1000] - Training Loss: 1.0436\n",
      "Epoch [535/1000] - Validation Loss: 1.0868\n",
      "Epoch [536/1000] - Training Loss: 1.0436\n",
      "Epoch [536/1000] - Validation Loss: 1.0869\n",
      "Epoch [537/1000] - Training Loss: 1.0436\n",
      "Epoch [537/1000] - Validation Loss: 1.0870\n",
      "Epoch [538/1000] - Training Loss: 1.0436\n",
      "Epoch [538/1000] - Validation Loss: 1.0871\n",
      "Epoch [539/1000] - Training Loss: 1.0436\n",
      "Epoch [539/1000] - Validation Loss: 1.0876\n",
      "Epoch [540/1000] - Training Loss: 1.0436\n",
      "Epoch [540/1000] - Validation Loss: 1.0876\n",
      "Epoch [541/1000] - Training Loss: 1.0436\n",
      "Epoch [541/1000] - Validation Loss: 1.0874\n",
      "Epoch [542/1000] - Training Loss: 1.0436\n",
      "Epoch [542/1000] - Validation Loss: 1.0873\n",
      "Epoch [543/1000] - Training Loss: 1.0436\n",
      "Epoch [543/1000] - Validation Loss: 1.0871\n",
      "Epoch [544/1000] - Training Loss: 1.0436\n",
      "Epoch [544/1000] - Validation Loss: 1.0871\n",
      "Epoch [545/1000] - Training Loss: 1.0436\n",
      "Epoch [545/1000] - Validation Loss: 1.0871\n",
      "Epoch [546/1000] - Training Loss: 1.0436\n",
      "Epoch [546/1000] - Validation Loss: 1.0874\n",
      "Epoch [547/1000] - Training Loss: 1.0436\n",
      "Epoch [547/1000] - Validation Loss: 1.0873\n",
      "Epoch [548/1000] - Training Loss: 1.0436\n",
      "Epoch [548/1000] - Validation Loss: 1.0873\n",
      "Epoch [549/1000] - Training Loss: 1.0436\n",
      "Epoch [549/1000] - Validation Loss: 1.0871\n",
      "Epoch [550/1000] - Training Loss: 1.0436\n",
      "Epoch [550/1000] - Validation Loss: 1.0870\n",
      "Epoch [551/1000] - Training Loss: 1.0436\n",
      "Epoch [551/1000] - Validation Loss: 1.0853\n",
      "Epoch [552/1000] - Training Loss: 1.0436\n",
      "Epoch [552/1000] - Validation Loss: 1.0857\n",
      "Epoch [553/1000] - Training Loss: 1.0436\n",
      "Epoch [553/1000] - Validation Loss: 1.0857\n",
      "Epoch [554/1000] - Training Loss: 1.0436\n",
      "Epoch [554/1000] - Validation Loss: 1.0860\n",
      "Epoch [555/1000] - Training Loss: 1.0436\n",
      "Epoch [555/1000] - Validation Loss: 1.0861\n",
      "Epoch [556/1000] - Training Loss: 1.0436\n",
      "Epoch [556/1000] - Validation Loss: 1.0863\n",
      "Epoch [557/1000] - Training Loss: 1.0436\n",
      "Epoch [557/1000] - Validation Loss: 1.0868\n",
      "Epoch [558/1000] - Training Loss: 1.0436\n",
      "Epoch [558/1000] - Validation Loss: 1.0869\n",
      "Epoch [559/1000] - Training Loss: 1.0436\n",
      "Epoch [559/1000] - Validation Loss: 1.0871\n",
      "Epoch [560/1000] - Training Loss: 1.0436\n",
      "Epoch [560/1000] - Validation Loss: 1.0871\n",
      "Epoch [561/1000] - Training Loss: 1.0436\n",
      "Epoch [561/1000] - Validation Loss: 1.0872\n",
      "Epoch [562/1000] - Training Loss: 1.0436\n",
      "Epoch [562/1000] - Validation Loss: 1.0873\n",
      "Epoch [563/1000] - Training Loss: 1.0436\n",
      "Epoch [563/1000] - Validation Loss: 1.0876\n",
      "Epoch [564/1000] - Training Loss: 1.0436\n",
      "Epoch [564/1000] - Validation Loss: 1.0875\n",
      "Epoch [565/1000] - Training Loss: 1.0436\n",
      "Epoch [565/1000] - Validation Loss: 1.0873\n",
      "Epoch [566/1000] - Training Loss: 1.0436\n",
      "Epoch [566/1000] - Validation Loss: 1.0874\n",
      "Epoch [567/1000] - Training Loss: 1.0436\n",
      "Epoch [567/1000] - Validation Loss: 1.0872\n",
      "Epoch [568/1000] - Training Loss: 1.0436\n",
      "Epoch [568/1000] - Validation Loss: 1.0865\n",
      "Epoch [569/1000] - Training Loss: 1.0436\n",
      "Epoch [569/1000] - Validation Loss: 1.0856\n",
      "Epoch [570/1000] - Training Loss: 1.0436\n",
      "Epoch [570/1000] - Validation Loss: 1.0859\n",
      "Epoch [571/1000] - Training Loss: 1.0436\n",
      "Epoch [571/1000] - Validation Loss: 1.0861\n",
      "Epoch [572/1000] - Training Loss: 1.0436\n",
      "Epoch [572/1000] - Validation Loss: 1.0864\n",
      "Epoch [573/1000] - Training Loss: 1.0436\n",
      "Epoch [573/1000] - Validation Loss: 1.0870\n",
      "Epoch [574/1000] - Training Loss: 1.0436\n",
      "Epoch [574/1000] - Validation Loss: 1.0876\n",
      "Epoch [575/1000] - Training Loss: 1.0436\n",
      "Epoch [575/1000] - Validation Loss: 1.0874\n",
      "Epoch [576/1000] - Training Loss: 1.0436\n",
      "Epoch [576/1000] - Validation Loss: 1.0872\n",
      "Epoch [577/1000] - Training Loss: 1.0436\n",
      "Epoch [577/1000] - Validation Loss: 1.0872\n",
      "Epoch [578/1000] - Training Loss: 1.0436\n",
      "Epoch [578/1000] - Validation Loss: 1.0873\n",
      "Epoch [579/1000] - Training Loss: 1.0436\n",
      "Epoch [579/1000] - Validation Loss: 1.0873\n",
      "Epoch [580/1000] - Training Loss: 1.0436\n",
      "Epoch [580/1000] - Validation Loss: 1.0874\n",
      "Epoch [581/1000] - Training Loss: 1.0436\n",
      "Epoch [581/1000] - Validation Loss: 1.0874\n",
      "Epoch [582/1000] - Training Loss: 1.0436\n",
      "Epoch [582/1000] - Validation Loss: 1.0874\n",
      "Epoch [583/1000] - Training Loss: 1.0436\n",
      "Epoch [583/1000] - Validation Loss: 1.0876\n",
      "Epoch [584/1000] - Training Loss: 1.0436\n",
      "Epoch [584/1000] - Validation Loss: 1.0877\n",
      "Epoch [585/1000] - Training Loss: 1.0436\n",
      "Epoch [585/1000] - Validation Loss: 1.0875\n",
      "Epoch [586/1000] - Training Loss: 1.0436\n",
      "Epoch [586/1000] - Validation Loss: 1.0875\n",
      "Epoch [587/1000] - Training Loss: 1.0436\n",
      "Epoch [587/1000] - Validation Loss: 1.0875\n",
      "Epoch [588/1000] - Training Loss: 1.0436\n",
      "Epoch [588/1000] - Validation Loss: 1.0875\n",
      "Epoch [589/1000] - Training Loss: 1.0436\n",
      "Epoch [589/1000] - Validation Loss: 1.0868\n",
      "Epoch [590/1000] - Training Loss: 1.0436\n",
      "Epoch [590/1000] - Validation Loss: 1.0859\n",
      "Epoch [591/1000] - Training Loss: 1.0436\n",
      "Epoch [591/1000] - Validation Loss: 1.0856\n",
      "Epoch [592/1000] - Training Loss: 1.0436\n",
      "Epoch [592/1000] - Validation Loss: 1.0857\n",
      "Epoch [593/1000] - Training Loss: 1.0436\n",
      "Epoch [593/1000] - Validation Loss: 1.0860\n",
      "Epoch [594/1000] - Training Loss: 1.0436\n",
      "Epoch [594/1000] - Validation Loss: 1.0861\n",
      "Epoch [595/1000] - Training Loss: 1.0436\n",
      "Epoch [595/1000] - Validation Loss: 1.0860\n",
      "Epoch [596/1000] - Training Loss: 1.0436\n",
      "Epoch [596/1000] - Validation Loss: 1.0860\n",
      "Epoch [597/1000] - Training Loss: 1.0436\n",
      "Epoch [597/1000] - Validation Loss: 1.0857\n",
      "Epoch [598/1000] - Training Loss: 1.0436\n",
      "Epoch [598/1000] - Validation Loss: 1.0853\n",
      "Epoch [599/1000] - Training Loss: 1.0436\n",
      "Epoch [599/1000] - Validation Loss: 1.0853\n",
      "Epoch [600/1000] - Training Loss: 1.0436\n",
      "Epoch [600/1000] - Validation Loss: 1.0863\n",
      "Epoch [601/1000] - Training Loss: 1.0436\n",
      "Epoch [601/1000] - Validation Loss: 1.0865\n",
      "Epoch [602/1000] - Training Loss: 1.0436\n",
      "Epoch [602/1000] - Validation Loss: 1.0846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [603/1000] - Training Loss: 1.0436\n",
      "Epoch [603/1000] - Validation Loss: 1.0844\n",
      "Epoch [604/1000] - Training Loss: 1.0436\n",
      "Epoch [604/1000] - Validation Loss: 1.0848\n",
      "Epoch [605/1000] - Training Loss: 1.0436\n",
      "Epoch [605/1000] - Validation Loss: 1.0855\n",
      "Epoch [606/1000] - Training Loss: 1.0436\n",
      "Epoch [606/1000] - Validation Loss: 1.0864\n",
      "Epoch [607/1000] - Training Loss: 1.0436\n",
      "Epoch [607/1000] - Validation Loss: 1.0864\n",
      "Epoch [608/1000] - Training Loss: 1.0436\n",
      "Epoch [608/1000] - Validation Loss: 1.0866\n",
      "Epoch [609/1000] - Training Loss: 1.0436\n",
      "Epoch [609/1000] - Validation Loss: 1.0867\n",
      "Epoch [610/1000] - Training Loss: 1.0436\n",
      "Epoch [610/1000] - Validation Loss: 1.0866\n",
      "Epoch [611/1000] - Training Loss: 1.0436\n",
      "Epoch [611/1000] - Validation Loss: 1.0869\n",
      "Epoch [612/1000] - Training Loss: 1.0436\n",
      "Epoch [612/1000] - Validation Loss: 1.0868\n",
      "Epoch [613/1000] - Training Loss: 1.0436\n",
      "Epoch [613/1000] - Validation Loss: 1.0868\n",
      "Epoch [614/1000] - Training Loss: 1.0436\n",
      "Epoch [614/1000] - Validation Loss: 1.0868\n",
      "Epoch [615/1000] - Training Loss: 1.0436\n",
      "Epoch [615/1000] - Validation Loss: 1.0867\n",
      "Epoch [616/1000] - Training Loss: 1.0436\n",
      "Epoch [616/1000] - Validation Loss: 1.0867\n",
      "Epoch [617/1000] - Training Loss: 1.0436\n",
      "Epoch [617/1000] - Validation Loss: 1.0868\n",
      "Epoch [618/1000] - Training Loss: 1.0436\n",
      "Epoch [618/1000] - Validation Loss: 1.0868\n",
      "Epoch [619/1000] - Training Loss: 1.0436\n",
      "Epoch [619/1000] - Validation Loss: 1.0872\n",
      "Epoch [620/1000] - Training Loss: 1.0436\n",
      "Epoch [620/1000] - Validation Loss: 1.0872\n",
      "Epoch [621/1000] - Training Loss: 1.0436\n",
      "Epoch [621/1000] - Validation Loss: 1.0872\n",
      "Epoch [622/1000] - Training Loss: 1.0436\n",
      "Epoch [622/1000] - Validation Loss: 1.0873\n",
      "Epoch [623/1000] - Training Loss: 1.0436\n",
      "Epoch [623/1000] - Validation Loss: 1.0873\n",
      "Epoch [624/1000] - Training Loss: 1.0436\n",
      "Epoch [624/1000] - Validation Loss: 1.0874\n",
      "Epoch [625/1000] - Training Loss: 1.0436\n",
      "Epoch [625/1000] - Validation Loss: 1.0881\n",
      "Epoch [626/1000] - Training Loss: 1.0436\n",
      "Epoch [626/1000] - Validation Loss: 1.0885\n",
      "Epoch [627/1000] - Training Loss: 1.0436\n",
      "Epoch [627/1000] - Validation Loss: 1.0881\n",
      "Epoch [628/1000] - Training Loss: 1.0436\n",
      "Epoch [628/1000] - Validation Loss: 1.0870\n",
      "Epoch [629/1000] - Training Loss: 1.0436\n",
      "Epoch [629/1000] - Validation Loss: 1.0867\n",
      "Epoch [630/1000] - Training Loss: 1.0436\n",
      "Epoch [630/1000] - Validation Loss: 1.0863\n",
      "Epoch [631/1000] - Training Loss: 1.0436\n",
      "Epoch [631/1000] - Validation Loss: 1.0862\n",
      "Epoch [632/1000] - Training Loss: 1.0436\n",
      "Epoch [632/1000] - Validation Loss: 1.0870\n",
      "Epoch [633/1000] - Training Loss: 1.0436\n",
      "Epoch [633/1000] - Validation Loss: 1.0869\n",
      "Epoch [634/1000] - Training Loss: 1.0436\n",
      "Epoch [634/1000] - Validation Loss: 1.0869\n",
      "Epoch [635/1000] - Training Loss: 1.0436\n",
      "Epoch [635/1000] - Validation Loss: 1.0868\n",
      "Epoch [636/1000] - Training Loss: 1.0436\n",
      "Epoch [636/1000] - Validation Loss: 1.0867\n",
      "Epoch [637/1000] - Training Loss: 1.0436\n",
      "Epoch [637/1000] - Validation Loss: 1.0878\n",
      "Epoch [638/1000] - Training Loss: 1.0436\n",
      "Epoch [638/1000] - Validation Loss: 1.0880\n",
      "Epoch [639/1000] - Training Loss: 1.0436\n",
      "Epoch [639/1000] - Validation Loss: 1.0878\n",
      "Epoch [640/1000] - Training Loss: 1.0436\n",
      "Epoch [640/1000] - Validation Loss: 1.0875\n",
      "Epoch [641/1000] - Training Loss: 1.0436\n",
      "Epoch [641/1000] - Validation Loss: 1.0872\n",
      "Epoch [642/1000] - Training Loss: 1.0436\n",
      "Epoch [642/1000] - Validation Loss: 1.0860\n",
      "Epoch [643/1000] - Training Loss: 1.0436\n",
      "Epoch [643/1000] - Validation Loss: 1.0856\n",
      "Epoch [644/1000] - Training Loss: 1.0436\n",
      "Epoch [644/1000] - Validation Loss: 1.0856\n",
      "Epoch [645/1000] - Training Loss: 1.0436\n",
      "Epoch [645/1000] - Validation Loss: 1.0865\n",
      "Epoch [646/1000] - Training Loss: 1.0436\n",
      "Epoch [646/1000] - Validation Loss: 1.0871\n",
      "Epoch [647/1000] - Training Loss: 1.0436\n",
      "Epoch [647/1000] - Validation Loss: 1.0869\n",
      "Epoch [648/1000] - Training Loss: 1.0436\n",
      "Epoch [648/1000] - Validation Loss: 1.0870\n",
      "Epoch [649/1000] - Training Loss: 1.0436\n",
      "Epoch [649/1000] - Validation Loss: 1.0869\n",
      "Epoch [650/1000] - Training Loss: 1.0436\n",
      "Epoch [650/1000] - Validation Loss: 1.0874\n",
      "Epoch [651/1000] - Training Loss: 1.0436\n",
      "Epoch [651/1000] - Validation Loss: 1.0875\n",
      "Epoch [652/1000] - Training Loss: 1.0436\n",
      "Epoch [652/1000] - Validation Loss: 1.0874\n",
      "Epoch [653/1000] - Training Loss: 1.0436\n",
      "Epoch [653/1000] - Validation Loss: 1.0876\n",
      "Epoch [654/1000] - Training Loss: 1.0436\n",
      "Epoch [654/1000] - Validation Loss: 1.0877\n",
      "Epoch [655/1000] - Training Loss: 1.0436\n",
      "Epoch [655/1000] - Validation Loss: 1.0877\n",
      "Epoch [656/1000] - Training Loss: 1.0436\n",
      "Epoch [656/1000] - Validation Loss: 1.0880\n",
      "Epoch [657/1000] - Training Loss: 1.0436\n",
      "Epoch [657/1000] - Validation Loss: 1.0879\n",
      "Epoch [658/1000] - Training Loss: 1.0436\n",
      "Epoch [658/1000] - Validation Loss: 1.0882\n",
      "Epoch [659/1000] - Training Loss: 1.0436\n",
      "Epoch [659/1000] - Validation Loss: 1.0879\n",
      "Epoch [660/1000] - Training Loss: 1.0436\n",
      "Epoch [660/1000] - Validation Loss: 1.0874\n",
      "Epoch [661/1000] - Training Loss: 1.0436\n",
      "Epoch [661/1000] - Validation Loss: 1.0870\n",
      "Epoch [662/1000] - Training Loss: 1.0436\n",
      "Epoch [662/1000] - Validation Loss: 1.0861\n",
      "Epoch [663/1000] - Training Loss: 1.0436\n",
      "Epoch [663/1000] - Validation Loss: 1.0859\n",
      "Epoch [664/1000] - Training Loss: 1.0436\n",
      "Epoch [664/1000] - Validation Loss: 1.0861\n",
      "Epoch [665/1000] - Training Loss: 1.0436\n",
      "Epoch [665/1000] - Validation Loss: 1.0862\n",
      "Epoch [666/1000] - Training Loss: 1.0436\n",
      "Epoch [666/1000] - Validation Loss: 1.0862\n",
      "Epoch [667/1000] - Training Loss: 1.0436\n",
      "Epoch [667/1000] - Validation Loss: 1.0857\n",
      "Epoch [668/1000] - Training Loss: 1.0436\n",
      "Epoch [668/1000] - Validation Loss: 1.0857\n",
      "Epoch [669/1000] - Training Loss: 1.0436\n",
      "Epoch [669/1000] - Validation Loss: 1.0861\n",
      "Epoch [670/1000] - Training Loss: 1.0436\n",
      "Epoch [670/1000] - Validation Loss: 1.0865\n",
      "Epoch [671/1000] - Training Loss: 1.0436\n",
      "Epoch [671/1000] - Validation Loss: 1.0867\n",
      "Epoch [672/1000] - Training Loss: 1.0436\n",
      "Epoch [672/1000] - Validation Loss: 1.0866\n",
      "Epoch [673/1000] - Training Loss: 1.0436\n",
      "Epoch [673/1000] - Validation Loss: 1.0854\n",
      "Epoch [674/1000] - Training Loss: 1.0436\n",
      "Epoch [674/1000] - Validation Loss: 1.0851\n",
      "Epoch [675/1000] - Training Loss: 1.0436\n",
      "Epoch [675/1000] - Validation Loss: 1.0847\n",
      "Epoch [676/1000] - Training Loss: 1.0436\n",
      "Epoch [676/1000] - Validation Loss: 1.0834\n",
      "Epoch [677/1000] - Training Loss: 1.0436\n",
      "Epoch [677/1000] - Validation Loss: 1.0836\n",
      "Epoch [678/1000] - Training Loss: 1.0436\n",
      "Epoch [678/1000] - Validation Loss: 1.0840\n",
      "Epoch [679/1000] - Training Loss: 1.0436\n",
      "Epoch [679/1000] - Validation Loss: 1.0846\n",
      "Epoch [680/1000] - Training Loss: 1.0436\n",
      "Epoch [680/1000] - Validation Loss: 1.0847\n",
      "Epoch [681/1000] - Training Loss: 1.0436\n",
      "Epoch [681/1000] - Validation Loss: 1.0848\n",
      "Epoch [682/1000] - Training Loss: 1.0436\n",
      "Epoch [682/1000] - Validation Loss: 1.0870\n",
      "Epoch [683/1000] - Training Loss: 1.0436\n",
      "Epoch [683/1000] - Validation Loss: 1.0847\n",
      "Epoch [684/1000] - Training Loss: 1.0436\n",
      "Epoch [684/1000] - Validation Loss: 1.0838\n",
      "Epoch [685/1000] - Training Loss: 1.0436\n",
      "Epoch [685/1000] - Validation Loss: 1.0836\n",
      "Epoch [686/1000] - Training Loss: 1.0436\n",
      "Epoch [686/1000] - Validation Loss: 1.0836\n",
      "Epoch [687/1000] - Training Loss: 1.0436\n",
      "Epoch [687/1000] - Validation Loss: 1.0826\n",
      "Epoch [688/1000] - Training Loss: 1.0436\n",
      "Epoch [688/1000] - Validation Loss: 1.0826\n",
      "Epoch [689/1000] - Training Loss: 1.0436\n",
      "Epoch [689/1000] - Validation Loss: 1.0827\n",
      "Epoch [690/1000] - Training Loss: 1.0436\n",
      "Epoch [690/1000] - Validation Loss: 1.0825\n",
      "Epoch [691/1000] - Training Loss: 1.0436\n",
      "Epoch [691/1000] - Validation Loss: 1.0830\n",
      "Epoch [692/1000] - Training Loss: 1.0436\n",
      "Epoch [692/1000] - Validation Loss: 1.0831\n",
      "Epoch [693/1000] - Training Loss: 1.0436\n",
      "Epoch [693/1000] - Validation Loss: 1.0833\n",
      "Epoch [694/1000] - Training Loss: 1.0436\n",
      "Epoch [694/1000] - Validation Loss: 1.0832\n",
      "Epoch [695/1000] - Training Loss: 1.0436\n",
      "Epoch [695/1000] - Validation Loss: 1.0833\n",
      "Epoch [696/1000] - Training Loss: 1.0436\n",
      "Epoch [696/1000] - Validation Loss: 1.0834\n",
      "Epoch [697/1000] - Training Loss: 1.0436\n",
      "Epoch [697/1000] - Validation Loss: 1.0837\n",
      "Epoch [698/1000] - Training Loss: 1.0436\n",
      "Epoch [698/1000] - Validation Loss: 1.0843\n",
      "Epoch [699/1000] - Training Loss: 1.0436\n",
      "Epoch [699/1000] - Validation Loss: 1.0842\n",
      "Epoch [700/1000] - Training Loss: 1.0436\n",
      "Epoch [700/1000] - Validation Loss: 1.0838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [701/1000] - Training Loss: 1.0436\n",
      "Epoch [701/1000] - Validation Loss: 1.0836\n",
      "Epoch [702/1000] - Training Loss: 1.0436\n",
      "Epoch [702/1000] - Validation Loss: 1.0835\n",
      "Epoch [703/1000] - Training Loss: 1.0436\n",
      "Epoch [703/1000] - Validation Loss: 1.0835\n",
      "Epoch [704/1000] - Training Loss: 1.0436\n",
      "Epoch [704/1000] - Validation Loss: 1.0834\n",
      "Epoch [705/1000] - Training Loss: 1.0436\n",
      "Epoch [705/1000] - Validation Loss: 1.0834\n",
      "Epoch [706/1000] - Training Loss: 1.0436\n",
      "Epoch [706/1000] - Validation Loss: 1.0832\n",
      "Epoch [707/1000] - Training Loss: 1.0436\n",
      "Epoch [707/1000] - Validation Loss: 1.0831\n",
      "Epoch [708/1000] - Training Loss: 1.0436\n",
      "Epoch [708/1000] - Validation Loss: 1.0831\n",
      "Epoch [709/1000] - Training Loss: 1.0436\n",
      "Epoch [709/1000] - Validation Loss: 1.0831\n",
      "Epoch [710/1000] - Training Loss: 1.0436\n",
      "Epoch [710/1000] - Validation Loss: 1.0830\n",
      "Epoch [711/1000] - Training Loss: 1.0436\n",
      "Epoch [711/1000] - Validation Loss: 1.0828\n",
      "Epoch [712/1000] - Training Loss: 1.0436\n",
      "Epoch [712/1000] - Validation Loss: 1.0829\n",
      "Epoch [713/1000] - Training Loss: 1.0436\n",
      "Epoch [713/1000] - Validation Loss: 1.0829\n",
      "Epoch [714/1000] - Training Loss: 1.0436\n",
      "Epoch [714/1000] - Validation Loss: 1.0828\n",
      "Epoch [715/1000] - Training Loss: 1.0436\n",
      "Epoch [715/1000] - Validation Loss: 1.0829\n",
      "Epoch [716/1000] - Training Loss: 1.0436\n",
      "Epoch [716/1000] - Validation Loss: 1.0830\n",
      "Epoch [717/1000] - Training Loss: 1.0436\n",
      "Epoch [717/1000] - Validation Loss: 1.0829\n",
      "Epoch [718/1000] - Training Loss: 1.0436\n",
      "Epoch [718/1000] - Validation Loss: 1.0828\n",
      "Epoch [719/1000] - Training Loss: 1.0436\n",
      "Epoch [719/1000] - Validation Loss: 1.0829\n",
      "Epoch [720/1000] - Training Loss: 1.0436\n",
      "Epoch [720/1000] - Validation Loss: 1.0831\n",
      "Epoch [721/1000] - Training Loss: 1.0436\n",
      "Epoch [721/1000] - Validation Loss: 1.0831\n",
      "Epoch [722/1000] - Training Loss: 1.0436\n",
      "Epoch [722/1000] - Validation Loss: 1.0831\n",
      "Epoch [723/1000] - Training Loss: 1.0436\n",
      "Epoch [723/1000] - Validation Loss: 1.0832\n",
      "Epoch [724/1000] - Training Loss: 1.0436\n",
      "Epoch [724/1000] - Validation Loss: 1.0830\n",
      "Epoch [725/1000] - Training Loss: 1.0436\n",
      "Epoch [725/1000] - Validation Loss: 1.0830\n",
      "Epoch [726/1000] - Training Loss: 1.0436\n",
      "Epoch [726/1000] - Validation Loss: 1.0824\n",
      "Epoch [727/1000] - Training Loss: 1.0436\n",
      "Epoch [727/1000] - Validation Loss: 1.0824\n",
      "Epoch [728/1000] - Training Loss: 1.0436\n",
      "Epoch [728/1000] - Validation Loss: 1.0827\n",
      "Epoch [729/1000] - Training Loss: 1.0436\n",
      "Epoch [729/1000] - Validation Loss: 1.0825\n",
      "Epoch [730/1000] - Training Loss: 1.0436\n",
      "Epoch [730/1000] - Validation Loss: 1.0823\n",
      "Epoch [731/1000] - Training Loss: 1.0436\n",
      "Epoch [731/1000] - Validation Loss: 1.0828\n",
      "Epoch [732/1000] - Training Loss: 1.0436\n",
      "Epoch [732/1000] - Validation Loss: 1.0831\n",
      "Epoch [733/1000] - Training Loss: 1.0436\n",
      "Epoch [733/1000] - Validation Loss: 1.0833\n",
      "Epoch [734/1000] - Training Loss: 1.0436\n",
      "Epoch [734/1000] - Validation Loss: 1.0833\n",
      "Epoch [735/1000] - Training Loss: 1.0436\n",
      "Epoch [735/1000] - Validation Loss: 1.0833\n",
      "Epoch [736/1000] - Training Loss: 1.0436\n",
      "Epoch [736/1000] - Validation Loss: 1.0838\n",
      "Epoch [737/1000] - Training Loss: 1.0436\n",
      "Epoch [737/1000] - Validation Loss: 1.0838\n",
      "Epoch [738/1000] - Training Loss: 1.0436\n",
      "Epoch [738/1000] - Validation Loss: 1.0832\n",
      "Epoch [739/1000] - Training Loss: 1.0436\n",
      "Epoch [739/1000] - Validation Loss: 1.0833\n",
      "Epoch [740/1000] - Training Loss: 1.0436\n",
      "Epoch [740/1000] - Validation Loss: 1.0834\n",
      "Epoch [741/1000] - Training Loss: 1.0436\n",
      "Epoch [741/1000] - Validation Loss: 1.0832\n",
      "Epoch [742/1000] - Training Loss: 1.0436\n",
      "Epoch [742/1000] - Validation Loss: 1.0835\n",
      "Epoch [743/1000] - Training Loss: 1.0436\n",
      "Epoch [743/1000] - Validation Loss: 1.0835\n",
      "Epoch [744/1000] - Training Loss: 1.0436\n",
      "Epoch [744/1000] - Validation Loss: 1.0833\n",
      "Epoch [745/1000] - Training Loss: 1.0436\n",
      "Epoch [745/1000] - Validation Loss: 1.0835\n",
      "Epoch [746/1000] - Training Loss: 1.0436\n",
      "Epoch [746/1000] - Validation Loss: 1.0833\n",
      "Epoch [747/1000] - Training Loss: 1.0436\n",
      "Epoch [747/1000] - Validation Loss: 1.0830\n",
      "Epoch [748/1000] - Training Loss: 1.0436\n",
      "Epoch [748/1000] - Validation Loss: 1.0830\n",
      "Epoch [749/1000] - Training Loss: 1.0436\n",
      "Epoch [749/1000] - Validation Loss: 1.0832\n",
      "Epoch [750/1000] - Training Loss: 1.0436\n",
      "Epoch [750/1000] - Validation Loss: 1.0829\n",
      "Epoch [751/1000] - Training Loss: 1.0436\n",
      "Epoch [751/1000] - Validation Loss: 1.0829\n",
      "Epoch [752/1000] - Training Loss: 1.0436\n",
      "Epoch [752/1000] - Validation Loss: 1.0824\n",
      "Epoch [753/1000] - Training Loss: 1.0436\n",
      "Epoch [753/1000] - Validation Loss: 1.0825\n",
      "Epoch [754/1000] - Training Loss: 1.0436\n",
      "Epoch [754/1000] - Validation Loss: 1.0828\n",
      "Epoch [755/1000] - Training Loss: 1.0436\n",
      "Epoch [755/1000] - Validation Loss: 1.0830\n",
      "Epoch [756/1000] - Training Loss: 1.0436\n",
      "Epoch [756/1000] - Validation Loss: 1.0831\n",
      "Epoch [757/1000] - Training Loss: 1.0436\n",
      "Epoch [757/1000] - Validation Loss: 1.0829\n",
      "Epoch [758/1000] - Training Loss: 1.0436\n",
      "Epoch [758/1000] - Validation Loss: 1.0829\n",
      "Epoch [759/1000] - Training Loss: 1.0436\n",
      "Epoch [759/1000] - Validation Loss: 1.0832\n",
      "Epoch [760/1000] - Training Loss: 1.0436\n",
      "Epoch [760/1000] - Validation Loss: 1.0839\n",
      "Epoch [761/1000] - Training Loss: 1.0436\n",
      "Epoch [761/1000] - Validation Loss: 1.0798\n",
      "Epoch [762/1000] - Training Loss: 1.0436\n",
      "Epoch [762/1000] - Validation Loss: 1.0802\n",
      "Epoch [763/1000] - Training Loss: 1.0436\n",
      "Epoch [763/1000] - Validation Loss: 1.0831\n",
      "Epoch [764/1000] - Training Loss: 1.0436\n",
      "Epoch [764/1000] - Validation Loss: 1.0843\n",
      "Epoch [765/1000] - Training Loss: 1.0436\n",
      "Epoch [765/1000] - Validation Loss: 1.0843\n",
      "Epoch [766/1000] - Training Loss: 1.0436\n",
      "Epoch [766/1000] - Validation Loss: 1.0842\n",
      "Epoch [767/1000] - Training Loss: 1.0436\n",
      "Epoch [767/1000] - Validation Loss: 1.0843\n",
      "Epoch [768/1000] - Training Loss: 1.0436\n",
      "Epoch [768/1000] - Validation Loss: 1.0817\n",
      "Epoch [769/1000] - Training Loss: 1.0436\n",
      "Epoch [769/1000] - Validation Loss: 1.0830\n",
      "Epoch [770/1000] - Training Loss: 1.0436\n",
      "Epoch [770/1000] - Validation Loss: 1.0831\n",
      "Epoch [771/1000] - Training Loss: 1.0436\n",
      "Epoch [771/1000] - Validation Loss: 1.0835\n",
      "Epoch [772/1000] - Training Loss: 1.0436\n",
      "Epoch [772/1000] - Validation Loss: 1.0835\n",
      "Epoch [773/1000] - Training Loss: 1.0436\n",
      "Epoch [773/1000] - Validation Loss: 1.0841\n",
      "Epoch [774/1000] - Training Loss: 1.0436\n",
      "Epoch [774/1000] - Validation Loss: 1.0842\n",
      "Epoch [775/1000] - Training Loss: 1.0436\n",
      "Epoch [775/1000] - Validation Loss: 1.0838\n",
      "Epoch [776/1000] - Training Loss: 1.0436\n",
      "Epoch [776/1000] - Validation Loss: 1.0837\n",
      "Epoch [777/1000] - Training Loss: 1.0436\n",
      "Epoch [777/1000] - Validation Loss: 1.0829\n",
      "Epoch [778/1000] - Training Loss: 1.0436\n",
      "Epoch [778/1000] - Validation Loss: 1.0825\n",
      "Epoch [779/1000] - Training Loss: 1.0436\n",
      "Epoch [779/1000] - Validation Loss: 1.0823\n",
      "Epoch [780/1000] - Training Loss: 1.0436\n",
      "Epoch [780/1000] - Validation Loss: 1.0818\n",
      "Epoch [781/1000] - Training Loss: 1.0436\n",
      "Epoch [781/1000] - Validation Loss: 1.0822\n",
      "Epoch [782/1000] - Training Loss: 1.0436\n",
      "Epoch [782/1000] - Validation Loss: 1.0823\n",
      "Epoch [783/1000] - Training Loss: 1.0436\n",
      "Epoch [783/1000] - Validation Loss: 1.0819\n",
      "Epoch [784/1000] - Training Loss: 1.0436\n",
      "Epoch [784/1000] - Validation Loss: 1.0819\n",
      "Epoch [785/1000] - Training Loss: 1.0436\n",
      "Epoch [785/1000] - Validation Loss: 1.0826\n",
      "Epoch [786/1000] - Training Loss: 1.0436\n",
      "Epoch [786/1000] - Validation Loss: 1.0834\n",
      "Epoch [787/1000] - Training Loss: 1.0436\n",
      "Epoch [787/1000] - Validation Loss: 1.0840\n",
      "Epoch [788/1000] - Training Loss: 1.0436\n",
      "Epoch [788/1000] - Validation Loss: 1.0842\n",
      "Epoch [789/1000] - Training Loss: 1.0436\n",
      "Epoch [789/1000] - Validation Loss: 1.0847\n",
      "Epoch [790/1000] - Training Loss: 1.0436\n",
      "Epoch [790/1000] - Validation Loss: 1.0848\n",
      "Epoch [791/1000] - Training Loss: 1.0436\n",
      "Epoch [791/1000] - Validation Loss: 1.0848\n",
      "Epoch [792/1000] - Training Loss: 1.0436\n",
      "Epoch [792/1000] - Validation Loss: 1.0845\n",
      "Epoch [793/1000] - Training Loss: 1.0436\n",
      "Epoch [793/1000] - Validation Loss: 1.0839\n",
      "Epoch [794/1000] - Training Loss: 1.0436\n",
      "Epoch [794/1000] - Validation Loss: 1.0836\n",
      "Epoch [795/1000] - Training Loss: 1.0436\n",
      "Epoch [795/1000] - Validation Loss: 1.0834\n",
      "Epoch [796/1000] - Training Loss: 1.0436\n",
      "Epoch [796/1000] - Validation Loss: 1.0833\n",
      "Epoch [797/1000] - Training Loss: 1.0436\n",
      "Epoch [797/1000] - Validation Loss: 1.0836\n",
      "Epoch [798/1000] - Training Loss: 1.0436\n",
      "Epoch [798/1000] - Validation Loss: 1.0836\n",
      "Epoch [799/1000] - Training Loss: 1.0436\n",
      "Epoch [799/1000] - Validation Loss: 1.0832\n",
      "Epoch [800/1000] - Training Loss: 1.0436\n",
      "Epoch [800/1000] - Validation Loss: 1.0832\n",
      "Epoch [801/1000] - Training Loss: 1.0436\n",
      "Epoch [801/1000] - Validation Loss: 1.0831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [802/1000] - Training Loss: 1.0436\n",
      "Epoch [802/1000] - Validation Loss: 1.0832\n",
      "Epoch [803/1000] - Training Loss: 1.0436\n",
      "Epoch [803/1000] - Validation Loss: 1.0838\n",
      "Epoch [804/1000] - Training Loss: 1.0436\n",
      "Epoch [804/1000] - Validation Loss: 1.0839\n",
      "Epoch [805/1000] - Training Loss: 1.0436\n",
      "Epoch [805/1000] - Validation Loss: 1.0837\n",
      "Epoch [806/1000] - Training Loss: 1.0436\n",
      "Epoch [806/1000] - Validation Loss: 1.0828\n",
      "Epoch [807/1000] - Training Loss: 1.0436\n",
      "Epoch [807/1000] - Validation Loss: 1.0822\n",
      "Epoch [808/1000] - Training Loss: 1.0436\n",
      "Epoch [808/1000] - Validation Loss: 1.0822\n",
      "Epoch [809/1000] - Training Loss: 1.0436\n",
      "Epoch [809/1000] - Validation Loss: 1.0824\n",
      "Epoch [810/1000] - Training Loss: 1.0436\n",
      "Epoch [810/1000] - Validation Loss: 1.0828\n",
      "Epoch [811/1000] - Training Loss: 1.0436\n",
      "Epoch [811/1000] - Validation Loss: 1.0816\n",
      "Epoch [812/1000] - Training Loss: 1.0436\n",
      "Epoch [812/1000] - Validation Loss: 1.0817\n",
      "Epoch [813/1000] - Training Loss: 1.0436\n",
      "Epoch [813/1000] - Validation Loss: 1.0823\n",
      "Epoch [814/1000] - Training Loss: 1.0436\n",
      "Epoch [814/1000] - Validation Loss: 1.0825\n",
      "Epoch [815/1000] - Training Loss: 1.0436\n",
      "Epoch [815/1000] - Validation Loss: 1.0824\n",
      "Epoch [816/1000] - Training Loss: 1.0436\n",
      "Epoch [816/1000] - Validation Loss: 1.0824\n",
      "Epoch [817/1000] - Training Loss: 1.0436\n",
      "Epoch [817/1000] - Validation Loss: 1.0826\n",
      "Epoch [818/1000] - Training Loss: 1.0436\n",
      "Epoch [818/1000] - Validation Loss: 1.0830\n",
      "Epoch [819/1000] - Training Loss: 1.0436\n",
      "Epoch [819/1000] - Validation Loss: 1.0834\n",
      "Epoch [820/1000] - Training Loss: 1.0436\n",
      "Epoch [820/1000] - Validation Loss: 1.0834\n",
      "Epoch [821/1000] - Training Loss: 1.0436\n",
      "Epoch [821/1000] - Validation Loss: 1.0825\n",
      "Epoch [822/1000] - Training Loss: 1.0436\n",
      "Epoch [822/1000] - Validation Loss: 1.0832\n",
      "Epoch [823/1000] - Training Loss: 1.0436\n",
      "Epoch [823/1000] - Validation Loss: 1.0838\n",
      "Epoch [824/1000] - Training Loss: 1.0436\n",
      "Epoch [824/1000] - Validation Loss: 1.0841\n",
      "Epoch [825/1000] - Training Loss: 1.0436\n",
      "Epoch [825/1000] - Validation Loss: 1.0838\n",
      "Epoch [826/1000] - Training Loss: 1.0436\n",
      "Epoch [826/1000] - Validation Loss: 1.0834\n",
      "Epoch [827/1000] - Training Loss: 1.0436\n",
      "Epoch [827/1000] - Validation Loss: 1.0837\n",
      "Epoch [828/1000] - Training Loss: 1.0436\n",
      "Epoch [828/1000] - Validation Loss: 1.0836\n",
      "Epoch [829/1000] - Training Loss: 1.0436\n",
      "Epoch [829/1000] - Validation Loss: 1.0839\n",
      "Epoch [830/1000] - Training Loss: 1.0436\n",
      "Epoch [830/1000] - Validation Loss: 1.0813\n",
      "Epoch [831/1000] - Training Loss: 1.0436\n",
      "Epoch [831/1000] - Validation Loss: 1.0825\n",
      "Epoch [832/1000] - Training Loss: 1.0436\n",
      "Epoch [832/1000] - Validation Loss: 1.0827\n",
      "Epoch [833/1000] - Training Loss: 1.0436\n",
      "Epoch [833/1000] - Validation Loss: 1.0828\n",
      "Epoch [834/1000] - Training Loss: 1.0436\n",
      "Epoch [834/1000] - Validation Loss: 1.0836\n",
      "Epoch [835/1000] - Training Loss: 1.0436\n",
      "Epoch [835/1000] - Validation Loss: 1.0844\n",
      "Epoch [836/1000] - Training Loss: 1.0436\n",
      "Epoch [836/1000] - Validation Loss: 1.0843\n",
      "Epoch [837/1000] - Training Loss: 1.0436\n",
      "Epoch [837/1000] - Validation Loss: 1.0843\n",
      "Epoch [838/1000] - Training Loss: 1.0436\n",
      "Epoch [838/1000] - Validation Loss: 1.0846\n",
      "Epoch [839/1000] - Training Loss: 1.0436\n",
      "Epoch [839/1000] - Validation Loss: 1.0846\n",
      "Epoch [840/1000] - Training Loss: 1.0436\n",
      "Epoch [840/1000] - Validation Loss: 1.0837\n",
      "Epoch [841/1000] - Training Loss: 1.0436\n",
      "Epoch [841/1000] - Validation Loss: 1.0838\n",
      "Epoch [842/1000] - Training Loss: 1.0436\n",
      "Epoch [842/1000] - Validation Loss: 1.0843\n",
      "Epoch [843/1000] - Training Loss: 1.0436\n",
      "Epoch [843/1000] - Validation Loss: 1.0850\n",
      "Epoch [844/1000] - Training Loss: 1.0436\n",
      "Epoch [844/1000] - Validation Loss: 1.0855\n",
      "Epoch [845/1000] - Training Loss: 1.0436\n",
      "Epoch [845/1000] - Validation Loss: 1.0853\n",
      "Epoch [846/1000] - Training Loss: 1.0436\n",
      "Epoch [846/1000] - Validation Loss: 1.0853\n",
      "Epoch [847/1000] - Training Loss: 1.0436\n",
      "Epoch [847/1000] - Validation Loss: 1.0852\n",
      "Epoch [848/1000] - Training Loss: 1.0436\n",
      "Epoch [848/1000] - Validation Loss: 1.0852\n",
      "Epoch [849/1000] - Training Loss: 1.0436\n",
      "Epoch [849/1000] - Validation Loss: 1.0850\n",
      "Epoch [850/1000] - Training Loss: 1.0436\n",
      "Epoch [850/1000] - Validation Loss: 1.0841\n",
      "Epoch [851/1000] - Training Loss: 1.0436\n",
      "Epoch [851/1000] - Validation Loss: 1.0842\n",
      "Epoch [852/1000] - Training Loss: 1.0436\n",
      "Epoch [852/1000] - Validation Loss: 1.0837\n",
      "Epoch [853/1000] - Training Loss: 1.0436\n",
      "Epoch [853/1000] - Validation Loss: 1.0840\n",
      "Epoch [854/1000] - Training Loss: 1.0436\n",
      "Epoch [854/1000] - Validation Loss: 1.0841\n",
      "Epoch [855/1000] - Training Loss: 1.0436\n",
      "Epoch [855/1000] - Validation Loss: 1.0843\n",
      "Epoch [856/1000] - Training Loss: 1.0436\n",
      "Epoch [856/1000] - Validation Loss: 1.0842\n",
      "Epoch [857/1000] - Training Loss: 1.0436\n",
      "Epoch [857/1000] - Validation Loss: 1.0843\n",
      "Epoch [858/1000] - Training Loss: 1.0436\n",
      "Epoch [858/1000] - Validation Loss: 1.0842\n",
      "Epoch [859/1000] - Training Loss: 1.0436\n",
      "Epoch [859/1000] - Validation Loss: 1.0846\n",
      "Epoch [860/1000] - Training Loss: 1.0436\n",
      "Epoch [860/1000] - Validation Loss: 1.0846\n",
      "Epoch [861/1000] - Training Loss: 1.0436\n",
      "Epoch [861/1000] - Validation Loss: 1.0869\n",
      "Epoch [862/1000] - Training Loss: 1.0436\n",
      "Epoch [862/1000] - Validation Loss: 1.0863\n",
      "Epoch [863/1000] - Training Loss: 1.0436\n",
      "Epoch [863/1000] - Validation Loss: 1.0846\n",
      "Epoch [864/1000] - Training Loss: 1.0436\n",
      "Epoch [864/1000] - Validation Loss: 1.0825\n",
      "Epoch [865/1000] - Training Loss: 1.0436\n",
      "Epoch [865/1000] - Validation Loss: 1.0826\n",
      "Epoch [866/1000] - Training Loss: 1.0436\n",
      "Epoch [866/1000] - Validation Loss: 1.0826\n",
      "Epoch [867/1000] - Training Loss: 1.0436\n",
      "Epoch [867/1000] - Validation Loss: 1.0831\n",
      "Epoch [868/1000] - Training Loss: 1.0436\n",
      "Epoch [868/1000] - Validation Loss: 1.0834\n",
      "Epoch [869/1000] - Training Loss: 1.0436\n",
      "Epoch [869/1000] - Validation Loss: 1.0843\n",
      "Epoch [870/1000] - Training Loss: 1.0436\n",
      "Epoch [870/1000] - Validation Loss: 1.0858\n",
      "Epoch [871/1000] - Training Loss: 1.0436\n",
      "Epoch [871/1000] - Validation Loss: 1.0861\n",
      "Epoch [872/1000] - Training Loss: 1.0436\n",
      "Epoch [872/1000] - Validation Loss: 1.0856\n",
      "Epoch [873/1000] - Training Loss: 1.0436\n",
      "Epoch [873/1000] - Validation Loss: 1.0850\n",
      "Epoch [874/1000] - Training Loss: 1.0436\n",
      "Epoch [874/1000] - Validation Loss: 1.0844\n",
      "Epoch [875/1000] - Training Loss: 1.0436\n",
      "Epoch [875/1000] - Validation Loss: 1.0848\n",
      "Epoch [876/1000] - Training Loss: 1.0436\n",
      "Epoch [876/1000] - Validation Loss: 1.0847\n",
      "Epoch [877/1000] - Training Loss: 1.0436\n",
      "Epoch [877/1000] - Validation Loss: 1.0849\n",
      "Epoch [878/1000] - Training Loss: 1.0436\n",
      "Epoch [878/1000] - Validation Loss: 1.0858\n",
      "Epoch [879/1000] - Training Loss: 1.0436\n",
      "Epoch [879/1000] - Validation Loss: 1.0860\n",
      "Epoch [880/1000] - Training Loss: 1.0436\n",
      "Epoch [880/1000] - Validation Loss: 1.0858\n",
      "Epoch [881/1000] - Training Loss: 1.0436\n",
      "Epoch [881/1000] - Validation Loss: 1.0852\n",
      "Epoch [882/1000] - Training Loss: 1.0436\n",
      "Epoch [882/1000] - Validation Loss: 1.0849\n",
      "Epoch [883/1000] - Training Loss: 1.0436\n",
      "Epoch [883/1000] - Validation Loss: 1.0842\n",
      "Epoch [884/1000] - Training Loss: 1.0436\n",
      "Epoch [884/1000] - Validation Loss: 1.0845\n",
      "Epoch [885/1000] - Training Loss: 1.0436\n",
      "Epoch [885/1000] - Validation Loss: 1.0845\n",
      "Epoch [886/1000] - Training Loss: 1.0436\n",
      "Epoch [886/1000] - Validation Loss: 1.0878\n",
      "Epoch [887/1000] - Training Loss: 1.0436\n",
      "Epoch [887/1000] - Validation Loss: 1.0877\n",
      "Epoch [888/1000] - Training Loss: 1.0436\n",
      "Epoch [888/1000] - Validation Loss: 1.0858\n",
      "Epoch [889/1000] - Training Loss: 1.0436\n",
      "Epoch [889/1000] - Validation Loss: 1.0835\n",
      "Epoch [890/1000] - Training Loss: 1.0436\n",
      "Epoch [890/1000] - Validation Loss: 1.0845\n",
      "Epoch [891/1000] - Training Loss: 1.0436\n",
      "Epoch [891/1000] - Validation Loss: 1.0804\n",
      "Epoch [892/1000] - Training Loss: 1.0436\n",
      "Epoch [892/1000] - Validation Loss: 1.0807\n",
      "Epoch [893/1000] - Training Loss: 1.0436\n",
      "Epoch [893/1000] - Validation Loss: 1.0817\n",
      "Epoch [894/1000] - Training Loss: 1.0436\n",
      "Epoch [894/1000] - Validation Loss: 1.0825\n",
      "Epoch [895/1000] - Training Loss: 1.0436\n",
      "Epoch [895/1000] - Validation Loss: 1.0833\n",
      "Epoch [896/1000] - Training Loss: 1.0436\n",
      "Epoch [896/1000] - Validation Loss: 1.0852\n",
      "Epoch [897/1000] - Training Loss: 1.0436\n",
      "Epoch [897/1000] - Validation Loss: 1.0850\n",
      "Epoch [898/1000] - Training Loss: 1.0436\n",
      "Epoch [898/1000] - Validation Loss: 1.0850\n",
      "Epoch [899/1000] - Training Loss: 1.0436\n",
      "Epoch [899/1000] - Validation Loss: 1.0847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [900/1000] - Training Loss: 1.0436\n",
      "Epoch [900/1000] - Validation Loss: 1.0852\n",
      "Epoch [901/1000] - Training Loss: 1.0436\n",
      "Epoch [901/1000] - Validation Loss: 1.0848\n",
      "Epoch [902/1000] - Training Loss: 1.0436\n",
      "Epoch [902/1000] - Validation Loss: 1.0846\n",
      "Epoch [903/1000] - Training Loss: 1.0436\n",
      "Epoch [903/1000] - Validation Loss: 1.0843\n",
      "Epoch [904/1000] - Training Loss: 1.0436\n",
      "Epoch [904/1000] - Validation Loss: 1.0829\n",
      "Epoch [905/1000] - Training Loss: 1.0436\n",
      "Epoch [905/1000] - Validation Loss: 1.0813\n",
      "Epoch [906/1000] - Training Loss: 1.0436\n",
      "Epoch [906/1000] - Validation Loss: 1.0819\n",
      "Epoch [907/1000] - Training Loss: 1.0436\n",
      "Epoch [907/1000] - Validation Loss: 1.0878\n",
      "Epoch [908/1000] - Training Loss: 1.0436\n",
      "Epoch [908/1000] - Validation Loss: 1.0885\n",
      "Epoch [909/1000] - Training Loss: 1.0436\n",
      "Epoch [909/1000] - Validation Loss: 1.0882\n",
      "Epoch [910/1000] - Training Loss: 1.0436\n",
      "Epoch [910/1000] - Validation Loss: 1.0882\n",
      "Epoch [911/1000] - Training Loss: 1.0436\n",
      "Epoch [911/1000] - Validation Loss: 1.0880\n",
      "Epoch [912/1000] - Training Loss: 1.0436\n",
      "Epoch [912/1000] - Validation Loss: 1.0890\n",
      "Epoch [913/1000] - Training Loss: 1.0436\n",
      "Epoch [913/1000] - Validation Loss: 1.0866\n",
      "Epoch [914/1000] - Training Loss: 1.0436\n",
      "Epoch [914/1000] - Validation Loss: 1.0829\n",
      "Epoch [915/1000] - Training Loss: 1.0436\n",
      "Epoch [915/1000] - Validation Loss: 1.0829\n",
      "Epoch [916/1000] - Training Loss: 1.0436\n",
      "Epoch [916/1000] - Validation Loss: 1.0834\n",
      "Epoch [917/1000] - Training Loss: 1.0436\n",
      "Epoch [917/1000] - Validation Loss: 1.0836\n",
      "Epoch [918/1000] - Training Loss: 1.0436\n",
      "Epoch [918/1000] - Validation Loss: 1.0840\n",
      "Epoch [919/1000] - Training Loss: 1.0436\n",
      "Epoch [919/1000] - Validation Loss: 1.0839\n",
      "Epoch [920/1000] - Training Loss: 1.0436\n",
      "Epoch [920/1000] - Validation Loss: 1.0839\n",
      "Epoch [921/1000] - Training Loss: 1.0436\n",
      "Epoch [921/1000] - Validation Loss: 1.0871\n",
      "Epoch [922/1000] - Training Loss: 1.0436\n",
      "Epoch [922/1000] - Validation Loss: 1.0865\n",
      "Epoch [923/1000] - Training Loss: 1.0436\n",
      "Epoch [923/1000] - Validation Loss: 1.0867\n",
      "Epoch [924/1000] - Training Loss: 1.0436\n",
      "Epoch [924/1000] - Validation Loss: 1.0862\n",
      "Epoch [925/1000] - Training Loss: 1.0436\n",
      "Epoch [925/1000] - Validation Loss: 1.0858\n",
      "Epoch [926/1000] - Training Loss: 1.0436\n",
      "Epoch [926/1000] - Validation Loss: 1.0856\n",
      "Epoch [927/1000] - Training Loss: 1.0436\n",
      "Epoch [927/1000] - Validation Loss: 1.0855\n",
      "Epoch [928/1000] - Training Loss: 1.0436\n",
      "Epoch [928/1000] - Validation Loss: 1.0825\n",
      "Epoch [929/1000] - Training Loss: 1.0436\n",
      "Epoch [929/1000] - Validation Loss: 1.0827\n",
      "Epoch [930/1000] - Training Loss: 1.0436\n",
      "Epoch [930/1000] - Validation Loss: 1.0834\n",
      "Epoch [931/1000] - Training Loss: 1.0436\n",
      "Epoch [931/1000] - Validation Loss: 1.0837\n",
      "Epoch [932/1000] - Training Loss: 1.0436\n",
      "Epoch [932/1000] - Validation Loss: 1.0837\n",
      "Epoch [933/1000] - Training Loss: 1.0436\n",
      "Epoch [933/1000] - Validation Loss: 1.0832\n",
      "Epoch [934/1000] - Training Loss: 1.0436\n",
      "Epoch [934/1000] - Validation Loss: 1.0838\n",
      "Epoch [935/1000] - Training Loss: 1.0436\n",
      "Epoch [935/1000] - Validation Loss: 1.0845\n",
      "Epoch [936/1000] - Training Loss: 1.0436\n",
      "Epoch [936/1000] - Validation Loss: 1.0850\n",
      "Epoch [937/1000] - Training Loss: 1.0436\n",
      "Epoch [937/1000] - Validation Loss: 1.0852\n",
      "Epoch [938/1000] - Training Loss: 1.0436\n",
      "Epoch [938/1000] - Validation Loss: 1.0844\n",
      "Epoch [939/1000] - Training Loss: 1.0436\n",
      "Epoch [939/1000] - Validation Loss: 1.0853\n",
      "Epoch [940/1000] - Training Loss: 1.0436\n",
      "Epoch [940/1000] - Validation Loss: 1.0857\n",
      "Epoch [941/1000] - Training Loss: 1.0436\n",
      "Epoch [941/1000] - Validation Loss: 1.0862\n",
      "Epoch [942/1000] - Training Loss: 1.0436\n",
      "Epoch [942/1000] - Validation Loss: 1.0863\n",
      "Epoch [943/1000] - Training Loss: 1.0436\n",
      "Epoch [943/1000] - Validation Loss: 1.0862\n",
      "Epoch [944/1000] - Training Loss: 1.0436\n",
      "Epoch [944/1000] - Validation Loss: 1.0861\n",
      "Epoch [945/1000] - Training Loss: 1.0436\n",
      "Epoch [945/1000] - Validation Loss: 1.0855\n",
      "Epoch [946/1000] - Training Loss: 1.0436\n",
      "Epoch [946/1000] - Validation Loss: 1.0855\n",
      "Epoch [947/1000] - Training Loss: 1.0436\n",
      "Epoch [947/1000] - Validation Loss: 1.0858\n",
      "Epoch [948/1000] - Training Loss: 1.0436\n",
      "Epoch [948/1000] - Validation Loss: 1.0857\n",
      "Epoch [949/1000] - Training Loss: 1.0436\n",
      "Epoch [949/1000] - Validation Loss: 1.0859\n",
      "Epoch [950/1000] - Training Loss: 1.0436\n",
      "Epoch [950/1000] - Validation Loss: 1.0862\n",
      "Epoch [951/1000] - Training Loss: 1.0436\n",
      "Epoch [951/1000] - Validation Loss: 1.0866\n",
      "Epoch [952/1000] - Training Loss: 1.0436\n",
      "Epoch [952/1000] - Validation Loss: 1.0868\n",
      "Epoch [953/1000] - Training Loss: 1.0436\n",
      "Epoch [953/1000] - Validation Loss: 1.0874\n",
      "Epoch [954/1000] - Training Loss: 1.0436\n",
      "Epoch [954/1000] - Validation Loss: 1.0876\n",
      "Epoch [955/1000] - Training Loss: 1.0436\n",
      "Epoch [955/1000] - Validation Loss: 1.0877\n",
      "Epoch [956/1000] - Training Loss: 1.0436\n",
      "Epoch [956/1000] - Validation Loss: 1.0869\n",
      "Epoch [957/1000] - Training Loss: 1.0436\n",
      "Epoch [957/1000] - Validation Loss: 1.0869\n",
      "Epoch [958/1000] - Training Loss: 1.0436\n",
      "Epoch [958/1000] - Validation Loss: 1.0859\n",
      "Epoch [959/1000] - Training Loss: 1.0436\n",
      "Epoch [959/1000] - Validation Loss: 1.0882\n",
      "Epoch [960/1000] - Training Loss: 1.0436\n",
      "Epoch [960/1000] - Validation Loss: 1.0858\n",
      "Epoch [961/1000] - Training Loss: 1.0436\n",
      "Epoch [961/1000] - Validation Loss: 1.0853\n",
      "Epoch [962/1000] - Training Loss: 1.0436\n",
      "Epoch [962/1000] - Validation Loss: 1.0857\n",
      "Epoch [963/1000] - Training Loss: 1.0436\n",
      "Epoch [963/1000] - Validation Loss: 1.0860\n",
      "Epoch [964/1000] - Training Loss: 1.0436\n",
      "Epoch [964/1000] - Validation Loss: 1.0864\n",
      "Epoch [965/1000] - Training Loss: 1.0436\n",
      "Epoch [965/1000] - Validation Loss: 1.0864\n",
      "Epoch [966/1000] - Training Loss: 1.0436\n",
      "Epoch [966/1000] - Validation Loss: 1.0862\n",
      "Epoch [967/1000] - Training Loss: 1.0436\n",
      "Epoch [967/1000] - Validation Loss: 1.0852\n",
      "Epoch [968/1000] - Training Loss: 1.0436\n",
      "Epoch [968/1000] - Validation Loss: 1.0851\n",
      "Epoch [969/1000] - Training Loss: 1.0436\n",
      "Epoch [969/1000] - Validation Loss: 1.0856\n",
      "Epoch [970/1000] - Training Loss: 1.0436\n",
      "Epoch [970/1000] - Validation Loss: 1.0862\n",
      "Epoch [971/1000] - Training Loss: 1.0436\n",
      "Epoch [971/1000] - Validation Loss: 1.0859\n",
      "Epoch [972/1000] - Training Loss: 1.0436\n",
      "Epoch [972/1000] - Validation Loss: 1.0862\n",
      "Epoch [973/1000] - Training Loss: 1.0436\n",
      "Epoch [973/1000] - Validation Loss: 1.0863\n",
      "Epoch [974/1000] - Training Loss: 1.0436\n",
      "Epoch [974/1000] - Validation Loss: 1.0865\n",
      "Epoch [975/1000] - Training Loss: 1.0436\n",
      "Epoch [975/1000] - Validation Loss: 1.0866\n",
      "Epoch [976/1000] - Training Loss: 1.0436\n",
      "Epoch [976/1000] - Validation Loss: 1.0866\n",
      "Epoch [977/1000] - Training Loss: 1.0436\n",
      "Epoch [977/1000] - Validation Loss: 1.0883\n",
      "Epoch [978/1000] - Training Loss: 1.0436\n",
      "Epoch [978/1000] - Validation Loss: 1.0882\n",
      "Epoch [979/1000] - Training Loss: 1.0436\n",
      "Epoch [979/1000] - Validation Loss: 1.0878\n",
      "Epoch [980/1000] - Training Loss: 1.0436\n",
      "Epoch [980/1000] - Validation Loss: 1.0897\n",
      "Epoch [981/1000] - Training Loss: 1.0436\n",
      "Epoch [981/1000] - Validation Loss: 1.0914\n",
      "Epoch [982/1000] - Training Loss: 1.0436\n",
      "Epoch [982/1000] - Validation Loss: 1.0925\n",
      "Epoch [983/1000] - Training Loss: 1.0436\n",
      "Epoch [983/1000] - Validation Loss: 1.0874\n",
      "Epoch [984/1000] - Training Loss: 1.0436\n",
      "Epoch [984/1000] - Validation Loss: 1.0859\n",
      "Epoch [985/1000] - Training Loss: 1.0436\n",
      "Epoch [985/1000] - Validation Loss: 1.0857\n",
      "Epoch [986/1000] - Training Loss: 1.0436\n",
      "Epoch [986/1000] - Validation Loss: 1.0843\n",
      "Epoch [987/1000] - Training Loss: 1.0436\n",
      "Epoch [987/1000] - Validation Loss: 1.0841\n",
      "Epoch [988/1000] - Training Loss: 1.0436\n",
      "Epoch [988/1000] - Validation Loss: 1.0839\n",
      "Epoch [989/1000] - Training Loss: 1.0436\n",
      "Epoch [989/1000] - Validation Loss: 1.0842\n",
      "Epoch [990/1000] - Training Loss: 1.0436\n",
      "Epoch [990/1000] - Validation Loss: 1.0843\n",
      "Epoch [991/1000] - Training Loss: 1.0436\n",
      "Epoch [991/1000] - Validation Loss: 1.0842\n",
      "Epoch [992/1000] - Training Loss: 1.0436\n",
      "Epoch [992/1000] - Validation Loss: 1.0852\n",
      "Epoch [993/1000] - Training Loss: 1.0436\n",
      "Epoch [993/1000] - Validation Loss: 1.0859\n",
      "Epoch [994/1000] - Training Loss: 1.0436\n",
      "Epoch [994/1000] - Validation Loss: 1.0859\n",
      "Epoch [995/1000] - Training Loss: 1.0436\n",
      "Epoch [995/1000] - Validation Loss: 1.0858\n",
      "Epoch [996/1000] - Training Loss: 1.0436\n",
      "Epoch [996/1000] - Validation Loss: 1.0848\n",
      "Epoch [997/1000] - Training Loss: 1.0436\n",
      "Epoch [997/1000] - Validation Loss: 1.0851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [998/1000] - Training Loss: 1.0436\n",
      "Epoch [998/1000] - Validation Loss: 1.0855\n",
      "Epoch [999/1000] - Training Loss: 1.0436\n",
      "Epoch [999/1000] - Validation Loss: 1.0862\n",
      "Epoch [1000/1000] - Training Loss: 1.0436\n",
      "Epoch [1000/1000] - Validation Loss: 1.0867\n"
     ]
    }
   ],
   "source": [
    "# Generate some example data (replace this with your actual data)\n",
    "input_size = 768\n",
    "num_classes = 6\n",
    "\n",
    "# Define the neural network model\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout_prob):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "hidden_size = 128\n",
    "dropout_prob = 0.3\n",
    "model = TwoLayerNN(input_size, hidden_size, num_classes, dropout_prob)\n",
    "\n",
    "mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "mlflow.log_param(\"learning_rate\", 0.001)\n",
    "mlflow.log_param('no of layers',2)\n",
    "mlflow.log_param('dropout',0.3)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    for batch_X, batch_y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        valid_loss = 0.0\n",
    "        for batch_X, batch_y in valid_dataloader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "            valid_loss += loss.item()\n",
    "    with torch.no_grad():\n",
    "        train_loss = 0.0\n",
    "        for batch_X, batch_y in train_dataloader:\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = valid_loss / len(valid_dataloader)\n",
    "        avg_train_loss = train_loss / len(train_dataloader)\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Training Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Validation Loss: {avg_valid_loss:.4f}\")\n",
    "        \n",
    "    \n",
    "    writer.add_scalar(\"Loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/valid\", valid_loss, epoch)\n",
    "    \n",
    "mlflow.log_metric(\"train_loss\", train_loss)\n",
    "mlflow.log_metric(\"valid_loss\", valid_loss)\n",
    "writer.flush()\n",
    "    \n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_valid)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "\n",
    "# You can use the predicted_classes for further analysis or evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mlflow.models.model.ModelInfo at 0x7f02a1342f40>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"model.pth\"\n",
    "torch.save(model, model_path)\n",
    "\n",
    "# Log the model as an artifact\n",
    "mlflow.pytorch.log_model(model, artifact_path=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.log_artifact(\"model.json\", artifact_path=\"model_architecture\")\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/bin/tensorboard\", line 6, in <module>\n",
      "    from tensorboard.main import run_main\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/tensorboard/main.py\", line 43, in <module>\n",
      "    from tensorboard import default\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/tensorboard/default.py\", line 36, in <module>\n",
      "    from tensorboard.plugins.audio import audio_plugin\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/tensorboard/plugins/audio/audio_plugin.py\", line 22, in <module>\n",
      "    from werkzeug import wrappers\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/werkzeug/__init__.py\", line 221, in <module>\n",
      "    from .test import Client\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/werkzeug/test.py\", line 29, in <module>\n",
      "    from .datastructures import CallbackDict\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/werkzeug/datastructures/__init__.py\", line 1, in <module>\n",
      "    from .accept import Accept as Accept\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/werkzeug/datastructures/accept.py\", line 6, in <module>\n",
      "    from .structures import ImmutableList\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/werkzeug/datastructures/structures.py\", line 1006, in <module>\n",
      "    from .. import http\n",
      "  File \"/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/werkzeug/http.py\", line 1241, in <module>\n",
      "    from .datastructures import Accept\n",
      "ImportError: cannot import name 'Accept' from partially initialized module 'werkzeug.datastructures' (most likely due to a circular import) (/home/vboxuser/mlprojects/sample/env/lib/python3.9/site-packages/werkzeug/datastructures/__init__.py)\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir='./runs' --port=6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inside the training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs = tokenizer(batch['text'], padding=True, truncation=True, return_tensors=\"pt\", max_length=max_length)\n",
    "        labels = batch['label']\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            bert_outputs = pretrained_model(**inputs).last_hidden_state\n",
    "        \n",
    "        cls_token_representation = bert_outputs[:, 0, :]\n",
    "        logits = custom_classifier(cls_token_representation)  # Pass [CLS] token's representation\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class TwoLayerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerClassifier, self).__init__()\n",
    "        self.hidden_layer = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        hidden_output = torch.relu(self.hidden_layer(x))\n",
    "        output = self.output_layer(hidden_output)\n",
    "        output_probs = self.softmax(output)\n",
    "        return output_probs\n",
    "\n",
    "# Define dimensions\n",
    "input_dim = 512\n",
    "hidden_dim = 256\n",
    "output_dim = 5\n",
    "\n",
    "# Create the model\n",
    "model = TwoLayerClassifier(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Define example data and labels\n",
    "data = torch.randn(100, input_dim)  # 100 samples\n",
    "labels = torch.randint(0, output_dim, (100,))  # 100 labels\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(data)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "print(\"Training finished!\")\n",
    "\n",
    "# Evaluate on new data\n",
    "new_data = torch.randn(20, input_dim)  # New 20 samples\n",
    "with torch.no_grad():\n",
    "    predictions = model(new_data)\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "    print(\"Predicted labels:\", predicted_labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
